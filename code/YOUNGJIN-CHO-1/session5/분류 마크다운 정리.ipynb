{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5915615-1fb6-4477-9e6d-5c56aad37598",
   "metadata": {},
   "source": [
    "# 분류 기초"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdccc06-cdbc-4558-993b-aa3a29cee27a",
   "metadata": {},
   "source": [
    "1. 분류란?\n",
    "\n",
    "  a.머신러닝\n",
    "\n",
    "   인공지능의 한 분야, 컴퓨터가 스스로 학습할 수 있도록 도와주는 알고리즘이나 기술을 개발하는 분야\n",
    "\n",
    "   알고리즘을 이용한 데이터 분석 -> 분석 결과 스스로 학습 -> 판단 또는 예측\n",
    "\n",
    "   지도학습 : 문제와 정답을 모두 알려주고 공부시키는 방법 -> 회귀, 분류\n",
    "\n",
    "   비지도학습 : 답을 가르쳐주지 않고 공부시키는 방법 -> 군집화 / 최근 집중적으로 연구되고 있음 / 정답이 없는 데이터의 특성 학습\n",
    "\n",
    "   강화학습 : 보상을 통해 상을 최대화, 벌을 최소화하는 방향으로 공부시키는 방법 / 시행착오를 반봅하여 정답을 찾는 것\n",
    "\n",
    "   -> 지도학습, 비지도학습의 궁극적인 목표 중 하나는 데이터 기반 미래 예측\n",
    "\n",
    "   cf. 비지도학습? 정답이 없는 데이터를 통해 패턴 또는 각 데이터 간 유사도를 기계가 학습하도록 하는 것 -> 주어진 데이터로만 학습 (feature만 존재), 구조나 패턴을 찾는 데 도움을 줌 (군집화, 밀도 추정, 차원 축소)\n",
    "\n",
    "   cf. 군집화? anomaly detection(변칙감지), 고객세분화(잠재고객을 여러 segment로) / 알고리즘이 식별해야 할 클러스터 수를 지정 또는 수정, 그룹을 어디까지 세분화할지 보다 쉽게 제어 가능\n",
    "\n",
    "\n",
    "   b. 지도학습 : 회귀와 분류\n",
    "\n",
    "   지도학습 : 입력 값과 함께 결과 값을 같이 주고 학습시키는 방법\n",
    "\n",
    "   학습의 방향 : 예측값과 정답이 최대한 같아지도록 학습\n",
    "\n",
    "   labeled data를 training set과 test set으로 나누고, training set을 통해 모델 학습, 독립변수를 통해 추정한 예측값이 주어진 종속변수와 같아지도록 모델 훈련 (기계의 예측이 우리가 의도한 정딥이 되도록 지도!)\n",
    "\n",
    "   정답을 기반으로 오류를 줄여 학습하는 방법(반복 학습 -> 오류를 줄여가며 정답에 가까워지는 방법)\n",
    "\n",
    "   연속적인 숫자값을 예측하는 '회귀' vs 입력된 데이터를 주어진 항목으로 나누는 '분류'\n",
    "\n",
    "   회귀 : 주어진 데이터(X)를 기반으로 정답(Y)를 잘 맞추는(fit) 함수(function) -> 연속형 변수를 예측하기 위해 사용!\n",
    "\n",
    "   분류 : 기존의 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 레이블을 판별하는 것 -> 범주형 변수를 예측하기 위해 사용!\n",
    "\n",
    "   예측하고자 하는 값(종속변수)에 따라 연속형이면 회귀, 범주형이면 분류를 사용!\n",
    "\n",
    "   분류에 쓰이는 대표적인 머신러닝 알고리즘\n",
    "\n",
    "   - 로지스틱 회귀 : 독립변수와 종속변수의 선형 관계성에 기반해 분류\n",
    "\n",
    "   - 결정 트리 : 데이터 균일도에 따른 규칙 기반의 분류\n",
    "\n",
    "   - 서포트 벡터 머신 : 개별 클래스 간의 최대 마진을 효과적으로 찾아 분류\n",
    "\n",
    "   - 최소 근접 알고리즘 : 근접 거리 기준으로 뷴류\n",
    "\n",
    "  c. 이진 분류 vs 다중 분류\n",
    "  \n",
    "  이진 분류 : 예측하고자 하는 변수가 어떤 기준에 대해 참 또는 거짓의 값만 가짐\n",
    "\n",
    "  다중 분류 : 예측하고자 하는 변수가 가질 수 있는 값이 3개 이상\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba440cd-59f3-499a-aef0-5bc8c6928e68",
   "metadata": {},
   "source": [
    "2. 분류 모델\n",
    "\n",
    "   a. 로지스틱 회귀 : 독립 변수의 선형 조합(선형 회귀)에 로지스틱 함수를 적용하여 출력값을 0에서 1 사이로 변환해주는 것\n",
    "\n",
    "   이진 분류 문제를 푸는 대표적인 알고리즘, 샘플이 특정 클래스에 속할 확률을 추정하는 것이 목표!\n",
    "\n",
    "   이진 분류를 다중 선형 회귀로 풀려고 했을 때 발생하는 문제 : 이진 분류 문제에 다중 선형 회귀를 적용할 경우 분류 작업이 제대로 동작하지 않음 (직선의 경우 분류 작업이 제대로 동작하지 않음)\n",
    "\n",
    "   이때 X와 y의 관계를 표현하기 위해서는 직선이 아닌 s자 형태를 표현할 수 있는 함수가 필요!\n",
    "\n",
    "   Why? 시그모이드 함수는 입력값이 작거나 클 때는 천천히 변함 -> 모델이 예측할 떄 급격한 변화를 방지하고, 현실적인 확률 값 제공하는 데 도움을 줌, 특정 독립 변수의 변화가 크지 않은 경우 시그모이드함수는 확률을 급격하게 바꾸지 않음 -> 노이즈에 민감하지 않은 안정적 예측 가능!\n",
    "   \n",
    "   예측값은 0과 1 사이의 값을 가져야 하는데, 직선으로 분류 작업을 수행하면 값이 음의 무한대부터 양의 무한대와 같은 큰 수들도 가질 수 있게 된다!\n",
    "\n",
    "   -> 출력이 0과 1 사이의 값을 가지면서 s자 형태로 그려지는 함수를 이용해야 한다!\n",
    "\n",
    "   시그모이드 함수 (=로지스틱 함수) : 출력이 0과 1 사이의 값을 가지면서 s자 형태로 그려지는 함수 / 이진 분류 작업에 적용 가능 / 입력값이 커지면 1에 수렴, 작아지면 0에 수렴 / 출력값이 특정 값 이상이면 1(True), 이하면 0(False)로 정하면 이진 분류 문제를 풀기 위해 사용할 수 있음\n",
    "\n",
    "   Sigmoid by code\n",
    "\n",
    "    def sigmoid(x):\n",
    "       return 1/(1+np.exp(-x))\n",
    "\n",
    "   x = np.arange(-5.0, 5.0, 0.1)\n",
    "\n",
    "   y = sigmoid(x)\n",
    "\n",
    "   plt.plot(x, y, 'g')\n",
    "\n",
    "   plt.plot([0, 0], [1.0, 0.0], ':')\n",
    "   \n",
    "   plt.title('Sigmoid Function')\n",
    "\n",
    "   plt.show()\n",
    "\n",
    "   - 시그모이드 함수의 가중치\n",
    "  \n",
    "   인공지능이 하는 것은 결국 주어진 데이터에 적합한 가중치 w와 b를 구하는 것\n",
    "\n",
    "   * 승산? 어떠한 사건이 발생할 확률이 발생하지 않을 확률보다 몇 배 더 높은지 알려줌\n",
    "  \n",
    "     로지스틱 회귀에서 승산을 왜 사용하는가? 해석을 더운 직관적으로 하기 위해! (비선형적 결과를 선형 결과로 변환하기 위해!) / 시그모이드 함수를 적용해 비선형성을 준 모델을 선형적으로 해석하기 위함 -> 로지스틱 회귀의 결과를 승산을 취해 log 함수를 취하면..(로짓 변환) -> 가중치의 해석이 훨씬 직관적으로 이뤄질 수 있다!\n",
    "\n",
    "   b. 결정 나무 : 조건에 따라 데이터 분류, 최종적으로 데이터가 순수한 label의 집합으로 구성될 때까지 분류를 반복하는 모델 like 스무고개\n",
    "\n",
    "    CART 알고리즘 : 가장 대표적인 결정 나무 알고리즘, 데이터셋을 임계값을 기준으로 두 child로 나누는 알고리즘 <- 임계값을 어떻게 나눌까? by 불순도(지니 계수)가 낮아지는 방향으로!\n",
    "\n",
    "        cf. 불순도? 분류하려는 데이터 집합에서 서로 다른 범주가 섞여 있는 정도, CART 알고리즘에서는 이를 지니계수(통계적 분산의 정도를 정량화해 표현한 값, 0~1)를 사용해 확인\n",
    "\n",
    "   1) 임계값 설정 : t1, .., t4, ...의 임계값을 기준으로 해 데이터를 그룹으로 나눔\n",
    "  \n",
    "   2) 불순도 감소 알고리즘 : 클래스를 정확하게 구분해줄 분류기준을 찾는 것이 중요! 불순도 알고리즘을 활용하여 현재 집단에 어느 정도 다른 객체들이 섞여 있는지 확인하고 불순도가 낮은 쪽으로 가지 형성\n",
    "  \n",
    "      cf. CART 알고리즘의 한계; Greedy Algorithm : CART 알고리즘은 당장의 지니계수를 낮추는 판단만 하여 효율적인 대안을 제시할 수 없다!(근시안적 알고리즘),\n",
    "\n",
    "    * 실제 학습 시 고려해야 할 것들 : 모수 설정, 차이점 시각화, Prunning\n",
    "  \n",
    "      1) Parameter 설정\n",
    "      2) 시각화\n",
    "      3) Prunning(가지치기), 불필요한 노드 지우기\n",
    "\n",
    "   c. 서포트 벡터 머신 : 클래스를 분류할 수 있는 다양한 경계선 중 최적의 라인을 찾아내는 알고리즘 -> 명확하게 분류할 수 있는 데이터 집단에서 뛰어난 성능을 보이며, 고차원의 공간(다수의 feature)에서도 효과적으로 사용 가능함\n",
    "\n",
    "   서포트 벡터 머신의 구성 : 서포트 벡터(구분하는 선과 가장 가까운 포인트) / 결정 경계(집단을 구분하는 선) / 마진(선과 각 점의 거리)\n",
    "\n",
    "   최적의 선을 찾는 방법? 결정 경계는 데이터로부터 가장 멀리 떨어져 있는 것이 좋음! -> 마진이 가장 큰 경우를 선택함으로써 최적의 선 찾을 수 있음\n",
    "\n",
    "   d. KNN : 비슷한 특성을 가진 데이터끼리 서로 가까이 있다는 점을 이용한 분류 알고리즘 like 유유상종; K개의 이웃 설정 -> 가까운 K개의 데이터를 기반으로 분류 \n",
    "\n",
    "    계산순서\n",
    "   1) 데이터 준비 : 미리 학습하는 과정이 없는 KNN, Just 데이터 준비/각 데이터는 특징 벡터와 레이블로 구성\n",
    "   2) K값 설정 : 원하는 K 값 설정, K는 가장 가까운 이웃의 개수, 보통 홀수로 설정\n",
    "   3) 거리 계산 : 새로운 데이터(예측하려는 데이터)가 주어지면 이 값과 기존 모든 데이터 간 거리를 계산(유클리드 거리, 맨해튼 거리 사용)\n",
    "   4) 가장 가까운 K개의 이웃 선택 : 계산된 거리 중 가장 작은 거리 값을 가진 K개의 데이터 선택, 이 데이터 포인트들이 \"가장 가까운 이웃\"\n",
    "   5) 분류하기 : K개의 이웃 중 가장 많이 등장하는 클래스가 예측의 결과!\n",
    "\n",
    "   KNN 특징 : 학습이 필요하지 않다는 것, 별도의 모델 없이 주변 데이터를 이용하여 분류\n",
    "   \n",
    "   장점 : 훈련 필요 X, 정보 손실 X\n",
    "\n",
    "   단점 : 쿼리를 처리하는 데 시간이 오래 소요(사전 계산 없이 그때그때 계산을 통해 구하기 때문!, 보통 test시점 연산이 적은 것이 유리), 강건하지 않음; 이상치에 큰 영향을 받음(Not Robust,데이터 간 거리 기반 분류 모델이기 때문)\n",
    "   \n",
    "   cf. 앙상블 : 여러 개별 분류 모델들을 결합해 하나의 분류 모델보다 더 좋은 성능을 내는 머신러닝 기법 / 여러 약 분류기를 병렬, 또는 직렬로 결합해 강 분류기 모델을 만드는 것 / 여러 모델을 학습시키고 각 모델의 예측 결과를 종합해 최종 예측!\n",
    "\n",
    "   앙상블 종류\n",
    "   1) 보팅 : 다른 알고리즘의 모델을 병렬로 사용\n",
    "   2) 배깅 : 동일 알고리즘의 모델을 병렬로 사용\n",
    "   3) 부스팅 : 동일 알고리즘의 모델을 직렬(순차적으로, sequential)로 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecd29a3-3293-47cf-af06-9b0353dfdf89",
   "metadata": {},
   "source": [
    "3. 분류 평가 지표\n",
    "\n",
    "   a. 혼동 행렬 : 분류 모델의 예측 결과를 정확한 예측과 잘못된 예측으로 구분하여 나타낸 표(행렬)\n",
    "\n",
    "   초록색 상자 : 예측을 올바르게 한 경우(예측 = 실제결과) / 참, 참(TP) / 거짓, 거짓(TN)\n",
    "\n",
    "   빨간색 상자 : 예측을 잘못한 경우(예측! = 실제결과) / 참, 거짓(FP) / 거짓, 참(FN)\n",
    "\n",
    "   혼동행렬을 이용한 분류모델 평가 지표\n",
    "   - 정확도 : 모든 가능한 예측 중 참인 비율 - 모델이 입력된 데이터에 대해 얼마나 정확하게 예측하는지 (TP + TN) / (TP + TN + FP + FN) -> 정답 레이블의 비율이 불균형하면 모델의 정확도를 신뢰할 수 없음\n",
    "\n",
    "   - 정밀도 : 참이라고 예측한 경우 실제 참의 비율, 거짓을 참으로 판단한 정도를 알 수 있음(TP / (TP + FP)) -> 관건은 FP!\n",
    "     \n",
    "   - 재현도 : 실제로 참인 경우 중 참으로 예측하는 비율, 참을 거짓으로 판단한 정도를 알 수 있음(TP / (TP + FN)) -> 관건은 FN!\n",
    "  \n",
    "     *정밀도와 재현도는 상충관계!-> 분류 시 확률에 기반해 Threshold가 넘으면 참, 미만이면 거짓으로 판단, 경계값을 조정하면 정밀도와 재현도를 조정할 수 있음 (경계값 낮춤 -> 참 예측 늘어남 -> 재현도 상승 / 경계값 높임 -> 참 예측 감소 -> 정밀도 상승) -> 두 Value 값이 만나는 지점은 Threshold로 정하면 예측 오류를 최소화할 수 있음! (임계값에 따라 포기하는 면적 상승, 임계값 설정이 매우 중요!)\n",
    "\n",
    "    그래프에서 각 점은 분류 모델이 해당 임계값에서 정밀도와 재현도를 어떻게 예측했는지 표시, 모든 임계값에 대해 점을 그려 모델의 예측 성능을 시각적으로 파악!\n",
    "\n",
    "   b. F1-score : 정밀도와 재현율의 조화평균, 머신러닝의 모델의 성능을 평가하는 주요 지표 (조화평균을 사용함으로써 정밀도와 재현율 간 균형을 효과정으로 평가할 수 있음)\n",
    "\n",
    "   c. ROC/AUC Curve\n",
    "\n",
    "    ROC Curve : 얼마나 분류가 잘 되었는가를 보여주는 그래프 (TPR : 참 중 참으로 예측한 비율 = recall / FPR : 거짓 중 참으로 잘못 예측한 비율)\n",
    "\n",
    "    어떻게 분류해도 만족할 수 없는 부분이 생김 -> 현실적 한계를 고려해 최선의 ROC Curve 도출해야 함!\n",
    "\n",
    "    AUC Curve : ROC와 x축 간 면적(적분 값) -> 모델의 성능을 숫자로! 0과 1 사이의 값, 1에 가까울수록 분류 성능 좋음!\n",
    "\n",
    "(지금까지는 모두 이진 분류 평가 지표)\n",
    "-----------------------------------------------------\n",
    "  \n",
    "   cf. 다중 분류 평가 지표 : 위 이진 분류 평가 지표를 사용해 클래스 별 점수를 구한 뒤, 이를 적절히 평균 내리는 것\n",
    "   1) Macro Average : 클래스 별로 구한 평가 지표 평균(동등한 가중치) -> 모든 라벨이 유사한 중요도를 가질 때\n",
    "   2) Weighted Average : 클래스 별로 구한 평가 지표 가중 평균(샘플 수에 따른 빈도가 높은 클래스에 큰 가중치 부여) -> 샘플이 많은 라벨에 중요도를 두고 싶을 때\n",
    "   3) Micro Average : 모든 클래스의 예측 결과를 더해 전체적인 성능을 평가하는 지표(동등한 가중치) -> 라벨에 상관없이 전체적인 성능을 평가하고 싶을 때"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589dfc9-9fa9-47fc-96c6-2b2af2585097",
   "metadata": {},
   "source": [
    "4. 하이퍼파라미터 최적화\n",
    "\n",
    "   a. 하이퍼파라미터 최적화\n",
    "\n",
    "    하이퍼파라미터 : 학습 시작 전 사용자가 직접 설정하는 변수, 모델 학습 과정에 반영되는 값\n",
    "\n",
    "    하이퍼파라미터 최적화 : 튜닝을 거쳐 적절한 하이퍼파라미터를 찾아 모델 성능을 향상시키는 것\n",
    "\n",
    "    하이퍼파라미터 최적화 과정\n",
    "   1) 하이퍼파라미터 탐색 범위 설정 : 최적 값을 찾고 싶은 하이퍼파라미터 범위 설정\n",
    "   2) 평가 지표 계산 함수(성능 평가 함수) 정의 : 탐색하려는 하이퍼파라미터를 인수로 받아 평가지표 값을 계산해주는 함수 정의)\n",
    "   3) 1단계에서 샘플링한 하이퍼파라미터 값을 사용하여 검증 데이터로 정확도 평가\n",
    "   4) 위 단계를 특정 횟수 반복하며, 정확도 결과를 보고 하이퍼파라미터의 범위 좁히기\n",
    "\n",
    "   b. 하이퍼파라미터 최적화 방법\n",
    "   1) Grid Search : 정해진 범위에서 하이퍼파라미터를 모두 순회(장점 : 범위가 넓고 단계가 작을수록 꼼꼼하게 전 범위 탐색하여 최적해를 정확히 찾을 수 있음 / 단점 : 시간이 오래 걸림) -> 넓은 범위, 큰 단계를 활용해 범위를 좁혀 사용\n",
    "   2) Random Search : 정해진 범위에서 하이퍼파라미터를 무작위로 탐색(장점 : 속도가 Grid Search보다 빠름 / 단점 : 무작위라는 한계 때문에 정확도 떨어짐, 사용 빈도 타 Search에 비해 낮음)\n",
    "   3) Bayesian Optimization : 사전 정보를 바탕으로 하이퍼파라미터 값을 확률적으로 추정하며 탐색(특징 : Gausain Process라는 통계학을 기반으로 만들어진 모델로, 여러 하이퍼 파라미터들에 대해 Aquisition Function을 적용했을 때 가장 큰 값이 나올 확률이 높은 지점을 찾아냄)\n",
    "  \n",
    "   cf. 하이퍼파라미터 최적화 검증\n",
    "   1) 훈련데이터 : 매개변수(가중치와 편향) 학습에 이용\n",
    "   2) 검증데이터 : 하이퍼파라미터 성능 평가에 이용\n",
    "   3) 테스트데이터 : 신경망의 범용 성능을 평가하는 데 이용\n",
    "  \n",
    "   cf. 하이퍼파라미터 최적화 자동화 프레임워크\n",
    "\n",
    "   Optuna 라이브러리를 활용해 자동화 프레임워크를 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b838d24-a30b-4292-aaf3-caef3e510e93",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

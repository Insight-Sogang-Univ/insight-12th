{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df183224-7e08-40fb-8a38-63cfb0918556",
   "metadata": {},
   "source": [
    "# 회귀 리뷰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0c5de-eb17-4677-b835-522043dc2a8a",
   "metadata": {},
   "source": [
    "- **지도학습** : <u>정답</u>으로 간주할 데이터 있음.\n",
    "- **비지도학습** : <u>정답</u>으로 간주할 데이터 없음.\n",
    "\n",
    "---\n",
    "\n",
    "- **회귀** : 연속적인 데이터에서 패턴(함수관계)을 찾아내는 통계적 방법.\n",
    "\t- 종류 - 선형/비선형 회귀, 로지스틱 회귀, 릿지 회귀, 라쏘 회귀, 다항 회귀\n",
    "\t- 활용 - 데이터 **요약**, **예측**, 시계열 모델링, 변수 간 인과관계 발견 등\n",
    "\t- 평가지표 - MSE, MAE, R-squre, Adjusted R-square, AIC, BIC(SC)\n",
    "\n",
    "---\n",
    "\n",
    "- **단순선형회귀분석**\n",
    "- **다중선형회귀분석**\n",
    "\n",
    "> 설명변수 개수가 다를 뿐, 둘은 본질적으로 같음.<br>\n",
    "> $\\to$ 이런 최적의 회귀선을 찾는 법(=fitting하는 법)은?\n",
    "\n",
    "$\\Rightarrow$ **최소제곱법, 최소자승법(Least Squares Method, Ordinary Least Square)**\n",
    "\n",
    "- = **OLS**\n",
    "- 회귀선($\\hat{y}$) - 관측된 데이터값($y$) = **잔차**.\n",
    "- **잔차의 제곱의 합(=RSS or SSR)** 을 최소화하는 회귀선을 찾는 방법."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d6c47e-c02a-4e26-9cc6-8d4c4c27ec68",
   "metadata": {},
   "source": [
    "# 1. 선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495a7c0-540e-4d4f-9a36-f1f69fd90582",
   "metadata": {},
   "source": [
    "## 1.1 다중선형회귀의 수식적 이해\n",
    "\n",
    "가장 일반적인 형태는 다음과 같다:\n",
    "\n",
    "$$y_i = \\beta_1 + \\beta_2 X_{2i} + \\beta_3 X_{3i} + ... + \\beta_k X_{ki} + \\epsilon_i$$\n",
    "\n",
    "종속변수 y에 대해 영향을 줄 것 같은 설명변수들을 선택해 선형적인 식을 작성한다.\n",
    "\n",
    "여기서 상수항과 오차항을 제외하면, 다음과 같이 가중평균을 구하는 식 형태가 되는 것을 확인할 수 있다.\n",
    "\n",
    "$$y = w_0 x_0 + w_1 x_1 + ... + w_m x_m = \\sum ^m _{i=0} w_i x_i = \\mathbf w ^T \\mathbf x$$\n",
    "\n",
    "$\\to$ 선형회귀식 = y에 대한 x들의 **가중평균**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76535476-a70f-4218-818b-1ea105fd8eab",
   "metadata": {},
   "source": [
    "## 1.2 다중선형회귀의 기본 가정\n",
    "\n",
    "선형회귀를 사용하기 위해 필요한 가정들이 있음 - 이들을 만족하지 않으면 분석 결과의 신뢰성이 떨어지거나 분석이 불가함.\n",
    "\n",
    "- 가정 1 : 선형성 - 종속변수와 설명 변수 간 관계가 선형적이어야 한다.\n",
    "\t- $E(y|x) = \\beta_1 + \\beta_2 X_i$\n",
    "- 가정 2 : 오차항의 평균은 0이다.\n",
    "\t- $E(\\epsilon_i X_i) = 0$\n",
    "\t- 백색 잡음(=오차항)의 성질을 만족하기 위해 필요한 가정.\n",
    "- 가정 3 : 독립성 - 각각의 설명 변수는 서로 선형독립적이야 한다.\n",
    "\t- $X_i, \\; X_j \\; \\text{is linearly independent for all} \\; i, j$\n",
    "\t- 이 가정이 위배될 경우 **다중공선성**이 있다고 함.\n",
    "- 가정 4 : 등분산성 - 오차항의 분산은 일정해야 한다.\n",
    "\t- $Var(\\epsilon_i | X_i) = \\sigma^2$ \n",
    "\t- 이 가정이 위배될 경우 **이분산성**이 있다고 함.\n",
    "\t\t- **횡단면 자료**에서 많이 나타남. \n",
    "\t\t- 이 경우 회귀 분석의 결과가 정확하지 않을 수 있음\n",
    "\t- 잔차의 도표화, 검정(Brown-Forsythe, Breusch-Pagan) 등을 통해 등분산성 확인.\n",
    "- 가정 5 : 오차항은 자기상관되어 있지 않다 ($\\to$ 오차항의 공분산은 항상 0이어야 함.)\n",
    "\t- $Cov(\\varepsilon_i , \\varepsilon_j | X_i) = 0$ \n",
    "\t- 이 가정이 위배될 경우 **자기상관**이 있다고 함. \n",
    "\t\t- 시계열 자료에서 많이 나타남.\n",
    "\t- 더빈-왓슨 검정, ACF, PACF 함수 등을 통해 자기상관 확인.\n",
    "- 가정 6 : 정규성 - 오차항이 정규분포를 따른다.\n",
    "\t- 위배되어도 다중선형회귀분석의 결과에 큰 영향 x.\n",
    "\t- 샤피로-윌크 검정, 자퀴-베라 검정, Q-Q plot으로 확인."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c4fee5-57ce-4e0e-9a64-f00b511c4f7c",
   "metadata": {},
   "source": [
    "## 1.3 회귀분석 평가 방법\n",
    "\n",
    "> 회귀분석의 결과의 유효성 평가를 위해, 회귀선을 데이터와 함께 **시각화**하거나 **통계지표**를 이용."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f2471-01c3-4d33-b205-f4d8c60d2711",
   "metadata": {},
   "source": [
    "### 시각화\n",
    "\n",
    "- 시각적으로 확인하기에는 좋음.\n",
    "- 두 회귀선 중 어떤 것이 해당 데이터를 더 잘 요약하는지 비교는 어려움.\n",
    "\n",
    "$\\to$ 객관적 비교를 위해 통계 지표 활용."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb91589-f3d7-44de-b302-e9d73a5fa504",
   "metadata": {},
   "source": [
    "### 통계지표\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "results = sm.OLS(Y, sm.add_constant(X)).fit()\n",
    "results.summary()\n",
    "```\n",
    "\n",
    "$\\to$ 이 코드를 통해 통계 지표 확인.\n",
    "\n",
    "**결정 계수(R-squared, $R^2$)**\n",
    "\n",
    "- 회귀분석에서 모델이 설명하는 데이터의 총 변동(=평균과의 차이) 중 설명된 비율을 나타냄.\n",
    "- 모델이 데이터를 얼마나 잘 설명하는지 측정하는 지표. 0과 1 사이의 값.\n",
    "\t- **1에 가까울수록** 모델이 데이터를 잘 설명함. (SSR, SST 값이 비슷, SSE는 0에 근접)\n",
    "- $$R^2 = \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}$$\n",
    "\t- SST : 총 편차\n",
    "\t- SSR : 회귀선에 의해 설명되는 편차\n",
    "\t- SSE : 회귀선에 의해 설명되지 않는 편차\n",
    "\n",
    "**조정된 결정 계수(Adjusted R-squared, Adjusted $R^2$)**\n",
    "\n",
    "> R-square 값이 높기만 하면 무조건 좋은 것은 아님 - $R^2$는 설명변수 개수가 증가하면 증가하므로, 데이터와 큰 관련 없는 변수를 추가해도 높아짐.\n",
    "\n",
    "- $R^2$를 변수의 개수 증가에 덜 민감하도록 조정한 지표.\n",
    "- 변수의 개수를 $k$라 할 때, 식에 $\\frac{n-1}{n-k}$를 곱해주면 됨. -- *변수의 개수에 따른 패널티를 부여함.*\n",
    "\n",
    "**AIC, BIC(SC)**\n",
    "\n",
    "- 정보기준(info. criteria), 값이 낮을수록 좋다고 평가.\n",
    "- AIC = Akaike Information Criterion\n",
    "- BIC(SC) = Bayesian Information Criterion\n",
    "\t- AIC보다 엄격한 기준. 데이터 양(n)에 따라 더 강한 패널티 부과."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2075f14-9409-4597-a95b-8d237da57f3b",
   "metadata": {},
   "source": [
    "# 2. 비선형 회귀\n",
    "> 데이터셋의 분포가 선형적이지 않을 경우 사용하는 회귀분석방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85aa262-102b-4771-9551-d1ca39dc543f",
   "metadata": {},
   "source": [
    "## 2.1 다항식 회귀모델\n",
    "\n",
    "- 선형 모델의 feature를 다항식으로 만들어 선형 회귀 사용.\n",
    "- 장점 : 선형 모델에 비해 회귀선을 잘 fit하도록 그릴 수 있음.\n",
    "- 단점 : 너무 많은 feature를 이용하면 과적합 문제 발생할 수도.\n",
    "\t- $\\to$ 이를 방지하기 위해 Ridge, Lasso 규제 이용 가능.\n",
    "\n",
    "```python\n",
    "# 1. 기존의 변수들을 다항식으로 만듦.\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures() # x1, x2를 2차항화.\n",
    "poly.fit([[2, 3]])\n",
    "poly = PolynomialFeatures(include_bias=False) # 상수항 제외\n",
    "poly.fit(train_input)\n",
    "test_poly = poly.transform(test_input)\n",
    "## poly.get_feature_names_out() # 변수명 출력\n",
    "```\n",
    "\n",
    "```python\n",
    "# 2. scikit-learn의 선형회귀 클래스 똑같이 이용.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train_poly, train_target)\n",
    "print(lr.score(train_poly, train_target))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb7e96c-0677-4f53-ba44-8bbc6647b906",
   "metadata": {},
   "source": [
    "## 2.2 지수/로그 회귀모델\n",
    "\n",
    "- 데이터의 증감 형태가 지수/로그함수 형태인 경우 $\\to$ 지수/로그 식을 이용해 회귀선 그음.\n",
    "- 2.1처럼 기존 식에 지수/로그를 사용해 변형한 후 선형 회귀 사용.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "plt.scatter(cancer.data[:,col1], cancer.data[:,col2], c= cancer.target, alpha=0.3)\n",
    "X = cancer.data[:,[col1]]\n",
    "y = np.log(cancer.data[:,col2]+0.01) # 값이 0인 데이터가 있어 0.01 을 더해줌.\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "xs = np.arange(0,30,0.1)\n",
    "ys = np.exp(xs*model.coef_[0] + model.intercept_) - 0.01\n",
    "\n",
    "plt.scatter(cancer.data[:,col1], cancer.data[:,col2], c= cancer.target, alpha=0.3)\n",
    "plt.plot(xs,ys,'r-',lw=3)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

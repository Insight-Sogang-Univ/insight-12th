{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f960b0dd",
   "metadata": {},
   "source": [
    "# 一.분류란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d794dbd",
   "metadata": {},
   "source": [
    "## (1) 머신러닝: 지도 학습과 비지도 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d69b61",
   "metadata": {},
   "source": [
    "### 머신러닝이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d4e8a7",
   "metadata": {},
   "source": [
    "> 인공지능의 한 분야로, 컴퓨터가 스스로 학습할 수 있도록 도와주는 알고리즘이나 기술을 개발하는 분야"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82cd90a",
   "metadata": {},
   "source": [
    "1. 알고리즘을 이용하여 데이터 분석\n",
    "2. 분석 결과를 스스로 학습\n",
    "3. 이를 기반으로 어떠한 판단이나 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386462d",
   "metadata": {},
   "source": [
    "### 지도 학습, 비지도 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c226cd5",
   "metadata": {},
   "source": [
    "> 지도 학습: 문제와 정답을 모두 알려주고 공부시키는 방법<br>\n",
    "> 비지도 학습: 답을 가르쳐주지 않고 공부시키는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a726183",
   "metadata": {},
   "source": [
    "지도 학습은 입력에 대한 정답이 존재하여 학습된 결과를 바탕으로 미지의 데이터에 대해 미래 값을 예측하는 방법이고<br>\n",
    "비지도 학습은 입력 데이터만 있고 정답이 없다. 정답을 찾는 것이 아닌 입력 데이터의 패턴, 특성을 발견하는 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a0edc",
   "metadata": {},
   "source": [
    "## (2) 지도 학습: 회귀와 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd0d50",
   "metadata": {},
   "source": [
    "### 지도 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d73b2a",
   "metadata": {},
   "source": [
    "- 예측값과 정답이 최대한 같아지도록 한다\n",
    "1. labeled data(독립변수와 종속변수)를 training set과 test set으로 나눈다\n",
    "2. training set을 통해 모델을 학습시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56b926",
   "metadata": {},
   "source": [
    "### 회귀와 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd2a57",
   "metadata": {},
   "source": [
    "> 지도 학습에는 대표적으로 연속적인 숫자값을 예측하는 회귀, 입력된 데이터를 주어진 항목들로 나누는 분류가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33feb6b3",
   "metadata": {},
   "source": [
    "- 회귀: 주어진 데이터(X)를 기반으로 정답(Y)를 잘 맞추는 함수\n",
    "- 분류: 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 레이블을 판별하는 것\n",
    "- 회귀와 분류의 차이: 회귀는 연속형 변수를 예측할 때 사용하고 분류는 범주형 변수를 예측할 때 사용한다. 즉, 회귀는 예측결과가 연속성을 띄지만, 분류는 연속성이 없다,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437044ea",
   "metadata": {},
   "source": [
    "### 분류에 쓰이는 대표적인 머신러닝 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bb4a9b",
   "metadata": {},
   "source": [
    "1. 로지스틱 회귀: 독립변수와 종속변수의 선형 관계성에 기반해 분류\n",
    "2. 결정 트리: 데이터 균일도에 따른 규칙 기반의 분류\n",
    "3. 서포트 벡터 머신(SVM): 개별 클래스 간의 최대 마진을 효과적으로 찾아 분류\n",
    "4. 최소 근접 알고리즘(K-NN): 근접 거리 기준으로 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4228cc5f",
   "metadata": {},
   "source": [
    "## (3) 이진 분류와 다중 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfedb5f6",
   "metadata": {},
   "source": [
    "- 이진 분류: 예측하고자 하는 변수가 어떤 기준에 대하여 참 또는 거짓의 값만을 가짐\n",
    "- 다중 분류: 예측하고자 하는 변수가 가질 수 있는 값이 3개 이상임 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56002803",
   "metadata": {},
   "source": [
    "# 二. 분류 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a8d9d4",
   "metadata": {},
   "source": [
    "## (1) 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf7a1e",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c445053",
   "metadata": {},
   "source": [
    "> 이진 분류 문제를 푸는 대표적인 알고리즘으로, 샘플이 특정 클래스에 속할 확률을 추정하는 것을 목표로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c4997",
   "metadata": {},
   "source": [
    "### 시그모이드 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ee388",
   "metadata": {},
   "source": [
    "- 출력이 0과 1사이의 값을 가지면서 S자 형태로 그려지는 함수\n",
    "- 입력값이 커지면 1에 수렴하고, 입력값이 작아지면 0에 수렴\n",
    "- 출력값이 0부터 1까지의 값을 갖ㅁ\n",
    "- 출력값이 특정값 이상이면 1(True), 특정값 이하면 0(False)로 정하면 이진 분류 문제를 풀기 위해서 사용 가능\n",
    "- 인공지능이 하는 것은, 결국 주어진 데이터에 적합한 가중치w와 b를 구하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ed9a8",
   "metadata": {},
   "source": [
    "## (2) 결정 트리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efca99d",
   "metadata": {},
   "source": [
    "### 결정 트리란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c08b5",
   "metadata": {},
   "source": [
    "> 조건에 따라 데이터를 분류하며, 최종적으로 데이터가 순수한 label의 집합으로 구성될 때까지 분류를 반복하는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a5fd3",
   "metadata": {},
   "source": [
    "### 결정 트리 용어 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cbb298",
   "metadata": {},
   "source": [
    "- Root Node: 결정 트리의 시작이 되는 노드\n",
    "- Edge: 노드와 노드를 연결하는 길목\n",
    "- LeafNodes: Tree의 가장 마지막 노드로, 모델에서 label에 해당\n",
    "- Height: Tree의 깊이로, 클수록 tree의 구조는 복잡해짐\n",
    "- Level: 노드의 절대적 위계, Root node의 level = 0, leaf node의 level = height -1\n",
    "- Parent: 상대적으로 높은 위계의 노드\n",
    "- Child: 상대적으로 낮은 위계의 노드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba9465",
   "metadata": {},
   "source": [
    "### CART 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df69560",
   "metadata": {},
   "source": [
    "- 가장 대표적인 결정 트리 알고리즘으로, 데이터셋을 임계값을 기준으로 두 child로 나누는 알고리즘 \n",
    "- 임계값은 불순도가 낮아지는 방향으로 나눠야 함\n",
    "- 불순도: 분류하려는 데이터 집합에서 서로 다른 클래스가 섞여 있는 정도를 의미\n",
    "- CART 알고리즘에서는 불순도를 확인하기 위해 지니 계수를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd83708",
   "metadata": {},
   "source": [
    "지니계수: 불순도를 나타내며, 통계적 분산 정도를 정량화해서 표현한 값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4cf8d",
   "metadata": {},
   "source": [
    "### 실제 학습 시 고려해야 할 것들: 모수 설정, 차이점 시각화, Prunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934b340c",
   "metadata": {},
   "source": [
    "1. parameter 설정\n",
    "- min_samples_split : 분할되기 위해 노드가 가져야 하는 최소 샘플\n",
    "- min_samples_leaf: 리프 노드가 가지고 있어야 하는 최소 샘플 수\n",
    "- min_weight_fraction leaf: min_samples_leaf에 가중치가 부여된 전체 샘플 수에서의 비율\n",
    "- max_leaf_nodes: 리프 노드의 최대 개수\n",
    "- max_features: 각 노드에서 분할에 사용할 특성의 최대 수\n",
    "2. 시각화\n",
    "- 시각화를 수행함으로써 분류가 잘 이루어졌는지 확인 가능\n",
    "3. Prunning\n",
    "- 불필요한 노드 지우기\n",
    "- 노드가 너무 많아지면 과적합이 될 확률이 높아짐\n",
    "- 하부 트리를 제거하여 일반화 성능을 높일 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd01b1a5",
   "metadata": {},
   "source": [
    "## (3) 서포트 벡터 머신(SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db926cd",
   "metadata": {},
   "source": [
    "### SVM이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119fe07",
   "metadata": {},
   "source": [
    "> 클래스를 분류할 수 있는 다양한 경계선 중 최적의 라인을 찾아내는 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320cc35",
   "metadata": {},
   "source": [
    "- 명확하게 분류할 수 있는 데이터 집단에서 뛰어난 성능을 보이며, 고차원 공간에서도 효과적으로 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ed8dd",
   "metadata": {},
   "source": [
    "### SVM의 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e14252",
   "metadata": {},
   "source": [
    "- Support vector: 구분하는 선과 가장 가까운 포인트\n",
    "- Decision Boundary: 집단을 구분하는 선\n",
    "- Margin: 선과 각 점의 거리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b0e1d9",
   "metadata": {},
   "source": [
    "- 결정 경계는 데이터로부터 가장 멀리 떨어져 있는게 좋음\n",
    "- Margin이 가장 큰 경우를 선택함으로써 최적의 선을 찾음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d826beb",
   "metadata": {},
   "source": [
    "## (4) KNN(K-Nearest Neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ff118",
   "metadata": {},
   "source": [
    "### KNN이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac560bb7",
   "metadata": {},
   "source": [
    "> 데이터로부터 거리가 가까운 k개의 다른 데이터 레이블을 참조하여 분류하는 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc41f3",
   "metadata": {},
   "source": [
    "### 계산 순서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ccabd2",
   "metadata": {},
   "source": [
    "1. 데이터 준비\n",
    "- 미리 학습하는 과정이 없기 때문에 데이터를 준비함\n",
    "- 각 데이터는 특징 벡터와 레이블로 이루어짐\n",
    "2. K 값 설정\n",
    "- K는 가장 가까운 이웃의 개수를 나타내며, 보통 홀수개로 설정\n",
    "3. 거리 계산\n",
    "- 새로운 데이터가 주어지면, 이 값과 기존 모든 데이터 간의 거리 계산\n",
    "- 이 때 유클리드 거리, 맨해튼 거리 등을 사용\n",
    "4. 가장 가까운 K개의 이웃 선택\n",
    "- 계산된 거리 중에서 가장 작은 거리 값을 가진 K개의 데이터를 선택\n",
    "- 이 데이터 포인트들이 가장 가까운 이웃\n",
    "5. 분류하기\n",
    "- K개의 이웃 중 가장 많이 등장하는 클래스가 예측 결과가 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f5715",
   "metadata": {},
   "source": [
    "### 장단점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5134bd",
   "metadata": {},
   "source": [
    "1. 장점\n",
    "- 훈련이 필요하지 않음\n",
    "- 정보 손실이 없음\n",
    "2. 단점\n",
    "- 쿼리를 처리하는 데에 시간이 오래 소요\n",
    "- 사전 학습을 한 모델을 사용하는 것이 아니기 때문\n",
    "- 이상치에 큰 영향을 받음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a75f3f",
   "metadata": {},
   "source": [
    "## (5) 앙상블"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a35b7e2",
   "metadata": {},
   "source": [
    "- 여러 개의 개별 분류 모델들을 결합해 하나의 분류 모델보다 더 좋은 성능을 내는 머신러닝 기법\n",
    "- 여러 약 분류기를 병렬 또는 직렬로 결합하여 강 분류기로 만드는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1823253",
   "metadata": {},
   "source": [
    "### 종류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa32623c",
   "metadata": {},
   "source": [
    "1. 보팅\n",
    "- 다른 알고리즘의 모델을 병렬로 사용\n",
    "2. 배깅\n",
    "- 동일 알고리즘의 모델을 병렬로 사용\n",
    "3. 부스팅\n",
    "- 동일 알고리즘의 모델을 직렬로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a07248a",
   "metadata": {},
   "source": [
    "# 三. 분류 평가 지표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c236e63",
   "metadata": {},
   "source": [
    "## (1) 혼동 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa944ce",
   "metadata": {},
   "source": [
    "> 혼동 행렬: 분류 모델의 예측 결과를 정확한 예측과 잘못된 예측으로 구분하여 나타낸 표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab96e3",
   "metadata": {},
   "source": [
    "- TP: 실제 참, 참으로 예측\n",
    "- TN: 실제 거짓, 거짓으로 예측\n",
    "- FP: 실제 거짓, 참으로 예측\n",
    "- FN: 실제 참, 거짓으로 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372bd874",
   "metadata": {},
   "source": [
    "### 혼동 행렬을 이용한 평가 지표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb397a",
   "metadata": {},
   "source": [
    "1. 정확도: 모든 가능한 예측 중 참인 비율\n",
    "- 단점: 정답 레이블의 비율이 불균형하면 모델의 정확도를 신뢰할 수 없음\n",
    "2. 정밀도: 참이라고 예측한 경우 실제 참의 비율\n",
    "3. 재현도: 실제로 참인 경우 중 참으로 예측하는 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5237f3",
   "metadata": {},
   "source": [
    "### 정밀도&재현도 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f9544",
   "metadata": {},
   "source": [
    "- 두 value값이 만나는 지점을 임계값으로 정하면, 예측 오류를 최소화 가능\n",
    "- 정밀도&재현도 그래프가 임계값 결정에 도움을 줄 수 있음\n",
    "- 모든 임계값에 대해 점을 그려서 모델의 예측 성능을 시각적으로 파악 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0312dbd1",
   "metadata": {},
   "source": [
    "## (2) F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e326904",
   "metadata": {},
   "source": [
    "> 정밀도와 재현율의 조화 평균<br>\n",
    " 머신러닝 모델의 성능을 평가하는 주요지표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52262c30",
   "metadata": {},
   "source": [
    "## (3) ROC/ AUC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8245a",
   "metadata": {},
   "source": [
    "1. <span style = \"color: blue\">ROC Curve</span>: 얼마나 분류가 잘 되었는가를 보여주는 그래프\n",
    "- TPR: 참인 것들 중에 참이라고 예측한 비율\n",
    "- FPR: 거짓인 것들 중에 참이라고 잘못 예측한 비율\n",
    "2. <span style = \"color: red\">AUC Curve</span>: ROC와 x축 사이의 면적\n",
    "- 1에 가까울수록 분류 성능이 좋은 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f127cf",
   "metadata": {},
   "source": [
    "## (4) 다중 분류 평가 지표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e8ed8",
   "metadata": {},
   "source": [
    "- 이진 분류 평가 지표를 사용해서 클래스 별로 점수를 구한 뒤, 이를 적절히 평균을 내리는 것\n",
    "- Macro average: 클래스별로 구한 평가 지표 평균\n",
    "- Weighted average: 클래스별로 구한 평가 지표 가중 평균\n",
    "- Micro average: 모든 클래스의 예측 결과를 더하여 전체적인 성능을 평가하는 지표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff117ec2",
   "metadata": {},
   "source": [
    "# 士. 하이퍼파라미터 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43ec30",
   "metadata": {},
   "source": [
    "## (1) 하이퍼파라미터 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71721400",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b97a5",
   "metadata": {},
   "source": [
    "- 하이퍼파라미터: 학습 시작 전에 사용자가 직접 설정하는 변수\n",
    "- ex) decision tree의 max_depth, random forest의 n_estimators\n",
    "- 하이퍼파라미터 최적화: 적절한 하이퍼파라미터를 찾아 모델 성능을 향상시키는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d331c7d",
   "metadata": {},
   "source": [
    "- 하이퍼파라미터 최적화 과정\n",
    "1. 하이퍼파라미터 탐색 범위 설정\n",
    "2. 평가 지표 계산 함수 정의\n",
    "3. 1단계에서 샘플링한 하이퍼파라미터값을 사용하여 검증 데이터로 정확도 평가\n",
    "4. 위 단계를 특정 횟수 반복하며, 정확도 결과를 보고 하이퍼파라미터 범위를 좁힘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5112697f",
   "metadata": {},
   "source": [
    "## (2) 하이퍼파라미터 최적화 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798eba7",
   "metadata": {},
   "source": [
    "#### 하이퍼 파라미터 최적화 방법\n",
    "1. Grid Search: 정해진 범위에서 하이퍼파라미터를 모두 순회\n",
    "- 장점: 범위가 넓고 정확하게 찾을 수 있음\n",
    "- 단점: 시간이 오래 걸림\n",
    "- 적용: 넓은 범위, 큰 스텝을 활용해 범위를 좁힘\n",
    "2. Random Search: 정해진 범위에서 하이퍼파라미터를 무작위로 탐색\n",
    "- 장점: 속도가 그리드 서치보다 빠름\n",
    "- 단점: 무작위라는 한계 때문에 정확도가 떨어짐\n",
    "3. Bayesian Optimization\n",
    "- 사전 정보를 바탕으로 최적 하이퍼파라미터값을 확률적으로 추정하며 탐색하는 기법\n",
    "- Aquisition Function을 적용했을 때 가장 큰 값이 나올 확률이 높은 지점을 찾아냄\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208abf7",
   "metadata": {},
   "source": [
    "#### 하이퍼파라미터 최적화 자동화 프레임워크"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5314aa",
   "metadata": {},
   "source": [
    "- Optuna를 활용하면 다양한 알고리즘의 파라미터를 효과적으로 탐색 가능\n",
    "- 교차 검증 기반으로 모델의 일반화 성능을 평가"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

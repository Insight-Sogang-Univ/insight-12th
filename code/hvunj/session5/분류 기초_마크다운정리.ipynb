{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbc52491-04ed-4ac3-94e7-0e8c51c85801",
   "metadata": {},
   "source": [
    "### 분류기초 마크다운 정리\n",
    "### 1. 분류란?\n",
    "#### (1) 머신러닝: 지도 학습과 비지도 학습\n",
    "**머신러닝** : 인공지능의 한 분야로, 컴퓨터가 스스로 학습할 수 있도록 도와주는 알고리즘이나 기술을 개발하는 분야\\\n",
    "알고리즘을 이용하여 데이터 분석하고 분석 결과를 스스로 학습하여 이를 기반으로 판단이나 예측\\\n",
    "\\\n",
    "**머신러닝의 종류**\n",
    "- 지도 학습: 문제와 정답을 모두 알려주고 공부시키는 방법\\\n",
    " -> 정답이 주어진 상태에서 학습하는 알고리즘 ex) 회귀, 분류\n",
    "- 비지도 학습: 답을 가르쳐주지 않고 공부시키는 방법\\\n",
    " -> 정답이 주어지지 않은 상태에서 학습하는 알고리즘 ex) 군집화(clustering)\n",
    "- 강화 학습: 보상을 통해 상을 최대화/ 벌을 최소화하는 방향으로 공부시키는 방법\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdbfe8e-cb58-4b9c-82ab-cedf5f64d364",
   "metadata": {},
   "source": [
    "*지도 학습과 비지도 학습의 차이*\\\n",
    "*학습을 할 때 정답이 주어지냐 주어지지 않냐의 차이*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d1fcc-764b-4553-9669-152ba6394eed",
   "metadata": {},
   "source": [
    "#### (2) 지도 학습: 회귀와 분류\n",
    "**[지도 학습]**\n",
    "* 구체적인 과정\n",
    "1) labeled data -> 독립변수와 종속변수가 모두 존재하는 데이터. 입력 데이터와 그에 대응하는 출력값이 있는 데이터를 의미\n",
    "2) labeled data를 training set과 test set으로 나눔\n",
    "3) training set을 통해 모델을 학습: 독립변수를 통해 추정한 예측값이 주어진 종속변수와 같아지도록 모델 훈련\\\n",
    "  -> 기계의 예측이 우리가 의도한 정답이 되도록 지도함\n",
    "\n",
    "**[회귀와 분류]**\n",
    "\n",
    "지도 학습에는 대표적으로 숫자값을 예측하는 **회귀**, 입력된 데이터를 주어진 항목들로 나누는 **분류**가 존재\\\n",
    "**회귀**: 주어진 데이터를 기반으로 정답을 잘 맞추는 함수를 찾는 문제\\\n",
    "**분류**: 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 레이블을 판별하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ffcffe-7f28-485f-af2c-f73c218e54b3",
   "metadata": {},
   "source": [
    "*회귀와 분류의 차이*\\\n",
    "*회귀는 데이터가 연속형 변수를 예측하기 위해 사용될 때, 분류는 데이터가 범주형 변수를 예측하기 위해 사용될 때*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5db7a3c-0a97-43b6-a0e5-80680e70b8e1",
   "metadata": {},
   "source": [
    "#### (3) 이진 분류와 다중 분류\n",
    "* **이진 분류**: 예측하고자 하는 변수가 어떤 기준에 대하여 참 또는 거짓의 값만을 가짐\n",
    "* **다중 분류**: 예측하고자 하는 변수가 가질 수 있는 값이 3개 이상임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd18e5d-95e5-4dc6-8317-86c5a59c3e98",
   "metadata": {},
   "source": [
    "### 2. 분류 모델\n",
    "#### (1) 로지스틱 회귀\n",
    "#### **로지스틱 회귀**\n",
    "로지스틱 회귀는 이진 분류 문제를 푸는 대표적인 알고리즘으로, 샘플이 특정 클래스에 속할 확률을 추정하는 것을 목표로 함\\\n",
    " -> 즉, 독립 변수의 선형 조합에 로지스틱 함수를 적용하여 출력값을 0에서 1사이로 변환해주는 것을 의미\\\n",
    " **[이진 분류를 다중 선형 회귀로 풀려고 했을 때 발생하는 문제]**\\\n",
    " 직선을 사용할 경우 보통 분류 작업이 제대로 동작하지 않음 -> 출력이 0과 1사이의 값을 가지면서 S자 형태로 그려지는 함수를 이용해야 함\n",
    "#### **시그모이드 함수**\n",
    "시그모이드 함수 = 로지스틱 함수: 출력이 0과 1사이의 값을 가지면서 S자 형태로 그려지는 함수\n",
    "* 입력값이 커지면 1에 수렴하고, 입력값이 작아지면 0에 수렴\n",
    "* 출력값이 0부터 1까지의 값을 가짐\n",
    "  -> 출력값이 특정값 이상이면 1, 특정값 이하면 0으로 정하면 이진 분류 문제를 풀기 위해서 사용할 수 있음\n",
    "  **[시그모이드 함수의 가중치]**\\\n",
    "  인공지능이 하는 것은, 결국 주어진 데이터에 적합한 가중치 w와 b를 구하는 것\n",
    "#### (2) 결정 나무\n",
    "**[결정 나무]**\n",
    "\n",
    "조건에 따라 데이터를 분류하며, 최종적으로 데이터가 순수한 label의 집합으로 구성될 때까지 분류를 반복하는 모델\\\n",
    "**[결정 나무 용어 정리]**\n",
    "* Root Node: Decision Tree의 시작이 되는 노드\n",
    "* Edge: 노드와 노드를 연결하는 길목\n",
    "* Leaf Nodes: Tree의 가장 마지막 노드로, 모델에서 label에 해당\n",
    "* Height(depth): Tree의 깊이로, 클수록 tree의 구조는 복잡해짐\n",
    "* Level: 노드의 절대적 위계, Rood node의 level=0, leaf node의 level=height-1\n",
    "* Parent: 상대적으로 높은 위계의 노드\n",
    "* Child: 상대적으로 낮은 위계의 노드\\\n",
    "\n",
    "**[CART 알고리즘]**\n",
    "\n",
    "CART: 가장 대표적인 결정 나무 알고리즘으로, 데이터셋을 임계값을 기준으로 두 child로 나누는 알고리즘\\\n",
    "-> 임계값은 불순도가 낮아지는 방향으로 나눔\\\n",
    "-> 불순도: 분류하려는 데이터 집합에서 서로 다른 클래스가 섞여 있는 정도를 의미, CART 알고리즘에서는 불순도를 확인하기 위해 \"지니 계수\" 사용\\\n",
    "-> 지니 계수: 불순도를 나타내며, 통계적 분산 정도를 정량화해서 표현한 값으로 0과 1사이\n",
    "#### (3) 서포트 벡터 머신 (SWM)\n",
    "**[서포트 벡터 머신]**\\\n",
    "클래스로 분류할 수 있는 다양한 경계선 중 최적의 라인을 찾아내는 알고리즘\\\n",
    " -> 명확하게 분류할 수 있는 데이터 집단에서 뛰어난 성능을 보이며, 고차원 공간에서도 효과적으로 사용 가능\\\n",
    "**[SVM의 구성]**\\\n",
    "* Support vector: 구분하는 선과 가장 가까운 포인트\n",
    "* Decision Boundary: 집단을 구분하는 선\n",
    "* Margin: 선과 각 점의 거리\\\n",
    "**[최적의 선을 찾는 방법]**\\\n",
    "  결정 경계는 데이터로부터 가장 멀리 떨어져 있는 것이 좋음. 따라서 SVM은 Margin이 가장 큰 경우를 선택함으로써 최적의 선을 찾을 수 있음\n",
    "#### (4) KNN (K-Nearest Neighbor)\n",
    "**[KNN]**\n",
    "\n",
    "데이터로부터 거리가 가까운k개의 다른 데이터 레이블을 참조하여 분류하는 알고리즘\\\n",
    ": K개의 이웃 설정 -> 가까운 K개의 데이터를 기반으로 분류\\\n",
    "**어떠한 학습이 필요하지 않음** -> 별도의 모델없이 데이터만을 이용해서 새로운 데이터가 왔을 때 그떄 즉석에서 주변 데이터를 이용하여 분류 하는 것\n",
    "### 3. 분류 평가 지표\n",
    "#### (1) 혼동 행렬\n",
    "**[혼동 행렬]**\n",
    "분류 모델의 예측 결과를 정확한 예측과 잘못된 예측으로 구분하여 나타낸 표\n",
    "#### (2) F1-Score\n",
    "**[F1-Score]**\n",
    "정밀도와 재현율의 조화 평균\n",
    "#### (3) ROC / AUC Curve\n",
    "\n",
    "**[ROC Curve]** 얼마나 분류가 잘 되었는 가를 보여주는 그래프\\\n",
    "**[AUC Curve]** ROC와 x축 사이의 면적(적분값)\n",
    "#### (4) 다중 분류 평가 지표\n",
    "\n",
    "이진 분류 평가 지표를 사용해서 클래스별로 점수를 구한 뒤, 이를 적절히 평균을 내리는 것\\\n",
    "**다중 분류 평가 지표**\n",
    "* Macro average: 클래스 별로 구한 평가 지표 평균\n",
    "* Weighted average: 클래스 별로 구한 평가 지표 가중 평균\n",
    "* Micro average: 모든 클래스의 예측 결과를 더하여 전체적인 성능을 평가하는 지표\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b371f65-02cf-4d3c-8d0b-9608d55fac01",
   "metadata": {},
   "source": [
    "### 4. 하이퍼파라미터 최적화\n",
    "#### (1) 하이퍼파라미터 최적화\n",
    "\n",
    "**[하이퍼 파라미터]**\n",
    "* 하이퍼파라미터: 학습 시작 전에 사용자가 직접 설정하는 변수\n",
    "* 하이퍼파라미터 최적화: 적절한 하이퍼파라미터를 찾아 모델 성능을 향상시키는 것\n",
    "\n",
    "**[하이퍼파라미터 최적화 과정]**\n",
    "  1) 하이퍼파라미터 탐색 범위 설정\n",
    "  2) 평가 지표 계산 함수 정의\n",
    "  3) 1단계에서 샘플링한 하이퍼파라미터 값을 사용하여 검증 데이터로 정확도 평가\n",
    "  4) 위 단계를 특정 횟수 반복하며, 정확도 결과를 보고 하이퍼파라미터의 범위 좁힘\n",
    " \n",
    "#### (2) 하이퍼파라미터 최적화 방법\n",
    "\n",
    "**[하이퍼파라미터 최적화 방법]**\n",
    "\n",
    "* Grid Search: 정해진 범위에서 하이퍼파라미터를 모두 순회\n",
    "* Random Search: 정해진 범위에서 하이퍼파라미터를 무작위로 탐색\n",
    "* Bayesian Optimization: 사전 정보를 바탕으로 하이퍼파라미터 값을 확률적으로 추정하며 탐색\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cbdb20-1383-4403-8b72-4d22180b2da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

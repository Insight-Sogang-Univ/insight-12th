{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c2613dd-b2d5-4071-85db-b2f1eeb3312e",
   "metadata": {},
   "source": [
    "# 분류_사전과제\n",
    "\n",
    "## 분류란?\n",
    ": 지도 학습(예측값을 미리 만들어 둔 정답과 같아지도록 기계를 학습시키는 것)의 일종. 궁극적 목표는 \"데이터 기반 미래 예측\"임. 비지도 학습은 정답 label이 없음.\n",
    "\n",
    "### 회귀 vs 분류\n",
    "회귀: 연속적인 숫자값 예측. 주어진 데이터 X를 기반으로 정답 Y를 잘 맞추는(Fit) 함수인 회귀선을 찾는 문제 => 연속형 범수 예측<br>\n",
    "분류: 입력된 데이터를 주어진 항목들로 나누기. 기존 데이터가 어떤 레이블에 속하는지에 관한 패턴을 알고리즘으로 인지한 후, 새롭게 관측된 데이터에 대한 레이블을 판별하는 것. => 범주형 변수 예측 / 로지스틱회귀, 결정트리, 서포트 벡터 머신, 최소근접 알고리즘 사용<br>\n",
    "\n",
    "### 이진 분류 VS 다중 분류\n",
    "이진 분류: 예측하고자 하는 변수가 T, F의 값만을 가짐.<br>\n",
    "다중 분류: 예측하고자 하는 변수가 가질 수 있는 값이 3개 이상임.\n",
    "\n",
    "## 분류 모델\n",
    "### 로지스틱회귀\n",
    ": **이진 분류** 문제를 푸는 알고리즘. 샘플이 특정 클래스에 속할 확률을 추정 => 출력값을 0~1사이로 변환\n",
    "\n",
    "- 직선은 분류 작업에 적합하지 않음. => 출력이 0과 1사이의값을 가지면서 S자 형태로 그려지는 함수 이용해야 함(시그모이드 함수(로지스틱 함수))\n",
    "- 시그모이드 함수: 입력값이 커지면 1에 수렴, 작아지면 0에 수렴. 따라서 어떤 출력값을 기준으로 분류 문제를 푸는 데 사용 가능함.\n",
    "- 시그모이드 함수의 가중치: 인공지능-> 주어진 데이터에 적합한 가중치 w와 b를 구하는 일을 함. \n",
    "\n",
    "### 결정 트리\n",
    ": 조건에 따라 데이터를 분류. 최종 데이터가순수한 label의 집합으로 구성될 때까지 분류를 반복함.\n",
    "- Root Node, Edge, Leaf Node, Height, Level, Parent, Childe\n",
    "- CART(Classification and Regression Tree) 알고리즘. -> 임계값을 기준으로 데이터셋을 두 child로 나눔.\n",
    "- 임계값은 불순도(지니 계수)가 낮아지는 방향으로 골라야 함. ❗분류하려는 데이터 집합에서 서로 다른 클래스가 섞여 있는 정도. 지니 계수는 해당 임계값이 얼마나 불순한지 알려줌.\n",
    "#### 과정\n",
    "1. 임계값 설정.\n",
    "2. 불순도 감소 알고리즘 -> 데이터를 어떤 기준으로 분류했을 때 동일한 객체들로만 잘 모아지도록 할 수 있을까? \n",
    "\n",
    "#### 실제 학습 시 고려 사항\n",
    "1. 파라미터 설정\n",
    "2. 차이점 시각화 -> 분류가 잘 되었나?3\n",
    "3. 가지치기(Prunning) -> 불필요한 노드 지우기(노드가 많아지면 과적합 확률 높음. 하부 트리를 제거하면 일반화 성능을 높일 수 있음. 단, 그 결과 깊이가 줄어들고, 결과의 개수가 줄어듦.  \n",
    "\n",
    "### 서포트 벡터 머신\n",
    ": 클래스를 분류할 수 있는 다양한 경계선 가운데 최적의 라인을 찾아내는 알고리즘 => 명확하게 분류할 수 있는 데이터 집단에서 뛰어난 성능. 고차원에서도 효과적으로 사용 가능.<br>\n",
    "\n",
    "1. 구성<br>\n",
    "- support vector: 구분하는 선과가장 가까운 포인트\n",
    "- Decision Boundary: 집단을 구분하는 선\n",
    "- Margin: 선과 각 점의 거리\n",
    "\n",
    "2. 최적의 선을 찾는 방법<br>\n",
    "- 결정 경계는 데이터로부터 가장 멀리 떨어져 있는 게 좋음. Margin 이 가장 큰 경우를 선택 = 최적\n",
    "\n",
    "### 최소근접 알고리즘\n",
    ": 유유상종 => 비슷한 특성을 가진 데이터끼리 서로 가까이 있다는 점 이용. k개의 이웃 설정 = 가까운 k개 데이터를 기반으로 분류한다.\n",
    "\n",
    "#### 계산 순서\n",
    "1. 데이터 준비: 특징 벡터, 레이블\n",
    "2. K값 설정. 보통 홀수\n",
    "3. 거리 계산. 새로운 데이터가 주어지면, 이 값과 기존 모든 데이터간 거리 계산\n",
    "4. 가장 가까운 K개의 이웃 선택. 계산된 거리 중에서 가장 작은 거리 값을 가진 K개의 데이터 선택. 이 데이터 포인트들이 \"가장 가까운 이웃\"임.\n",
    "5. 분류. K개의 이웃 중 가장 많이 등장하는 클래스 = 예측 결과\n",
    "\n",
    "KNN은 별도의 학습 없이, 데이터만을 이용해서, 주변데이터와의 거리를 측정해서 classify\n",
    "\n",
    "장점: 훈련x, 정보 손실 x\n",
    "단점: 쿼리 처리에 시간적 비용이 큼(새로운 데이터에 대해서 계산 다시함). 이상치에 큰 영향을 받음.\n",
    "\n",
    "## 분류 평가 지표\n",
    "1. 혼동 행렬\n",
    ": 분류 모델의 예측 결과를 정확한 예측과 잘못된 예측으로 구분하여 나타낸 표(행렬)\n",
    "\n",
    "- 예측 성공 여부 + 예측값 참/거짓 => Total 4가지 경우의 수\n",
    "- 정확도: 모든 가능한 예측 충 참인 비율\n",
    "- 정밀도: 참이라고 예측한 경우 실제 참의 비율\n",
    "- 재현도: 실제로 참인 경우 중 참으로 예측하는 비율\n",
    "\n",
    "Threshold -> Precision과 Recall을 조정할 수 있다.\n",
    "\n",
    "정밀도 & 재현도 그래프<br>\n",
    ": 두 value 값이 만나는지점을 Threshold로 정하면, 예측 오류를 최소화할 수 있다. \n",
    "\n",
    "\n",
    "\n",
    "2. F1-Score\n",
    ": 머신러닝 모델의 성능 평가 지표. 조화평균을사용하는 이유는 정밀도와 재현간의 균형을 효과적으로 평가하기 위함.\n",
    "\n",
    "3. ROC / AUC Curve\n",
    ": 분류 모델의 성능을 평가하는 중요한 도구.<br>\n",
    "ROC Curve: 얼마나 분류가 잘 되었는가를 보여줌. True Positive Rate: 참/참 / False Positive Rate: 참/거짓\n",
    "=> 현식적으로 타협해서 최선의 ROC Curve를 도출해야 함.\n",
    "\n",
    "AOC Curve: ROC와 x축 사이의 면적(적분값) -> 0~1사이 값. 1에 가까울수록 분류 성능이 좋다.\n",
    "\n",
    "## 하이퍼파라미터 최적화\n",
    "하이퍼파라미터란? 모델이 학습하면서 최적의 값을 자동으로 찾는 것이 아닌, 사람이 직접 지정해야 하는 변수를 가리키는 용어이다.\n",
    "\n",
    "1. Grid Search: 정해진 범위에서 하이퍼파라미터를 모두 순회<br>\n",
    "- 장: 최적해를 정확히 찾음\n",
    "- 단: 시간이 오래 걸림.\n",
    "- 넓은 범위, 큰 step을 사용->범위를 좁힘.\n",
    "\n",
    "2. Random Search:  정해진 범위에서 하이퍼파라미터를 무작위로 탐색\n",
    "- 장: 속도가 Grid Search보다 빠름\n",
    "- 단: 무작위 -> 정확도 떨어짐.\n",
    "\n",
    "\n",
    "3. Bayesian Optimization: 사전 정보를 바탕으로 하이퍼파라미터 값을 확률적으로 추정하며 탐색\n",
    "- 여러 개의 하이퍼파라미터에 대해서 Aquisition Function 을 적용했을 때, 가장 큰 값이 나올 확률이 높은 지점을 찾아냄. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad212d52-a13f-4be7-9a85-498ea281df0c",
   "metadata": {},
   "source": [
    "### Session 4 : HomeWork\n",
    "\n",
    "- contents\n",
    "    \n",
    "    [ 문제 상황 ]\n",
    "    \n",
    "    미국 시장에서 이러한 공유 자전거의 수요에 영향을 미치는 요인을 이해해보자. \n",
    "    \n",
    "    공유자전거의 수요를 예측하는 데 중요한 변수는 무엇일까? 이 변수들이 자전거 요구를 얼마나 잘 설명하는지를 보여준다. 다양한 기상 조사와 사람들의 스타일을 기반으로 서비스 제공업체는 일부 요인을 기반으로 미국 시장 전반의 일상적인 자전거 수요에 대한 대규모 데이터 세트를 수집했습니다. \n",
    "    \n",
    "    [ 목표 ]\n",
    "    \n",
    "    사용가능한 독립 변수를 사용하여 공유 자전거에 대한 수요를 모델링해야 한다.\n",
    "    \n",
    "    - project\n",
    "        1. **데이터 불러오기**\n",
    "        - contents\n",
    "            \n",
    "            ```python\n",
    "            import warnings \n",
    "            warnings.filterwarnings('ignore')\n",
    "            import numpy as np\n",
    "            import pandas as pd\n",
    "            import matplotlib.pyplot as plt\n",
    "            import seaborn as sns\n",
    "            %matplotlib inline\n",
    "            \n",
    "            bike = pd.DataFrame(pd.read_csv(\"day.csv\"))\n",
    "            # check the head of dataset\n",
    "            bike.head()\n",
    "            # check the descriptive information\n",
    "            bike.info()\n",
    "            # check statistical indicators\n",
    "            bike.describe()\n",
    "            # Check the shape of df : num of cols, rows 판단\n",
    "            print(bike.shape)\n",
    "            ```\n",
    "            \n",
    "            코드를 실행시켜본 결과 :\n",
    "            \n",
    "            dteday열을 제외한 나머지 열은 모두 모두 float와 integer 형식이다. \n",
    "            \n",
    "            dteday 열은 날짜 유형이다.\n",
    "            \n",
    "            데이터를 보면 범주형이지만 integer/float type 필드가 있다. \n",
    "            \n",
    "        1. **Data Quality Check**\n",
    "        - contents\n",
    "            1. 결측값 확인\n",
    "            \n",
    "            ```python\n",
    "            # 열에 대한 결측값의 비율 계산 후 내림차순 정렬\n",
    "            round(100*(bike.isnull().sum()/len(bike)),2).sort_values(ascending=False)\n",
    "            # 행에 대한 결측값의 비율 계산 후 내림차순 정렬\n",
    "            round((bike.isnull().sum(axis=1)/len(bike))*100,2).sort_values(ascending=False)\n",
    "            ```\n",
    "            \n",
    "            열과 행에 결측값이 없다.\n",
    "            \n",
    "            1. 중복 확인\n",
    "            \n",
    "            ```python\n",
    "            bike_dup = bike.copy()\n",
    "            # 중복값을 확인하고 중복값을 제거\n",
    "            bike_dup.drop_duplicates(subset=None,inplace=True)\n",
    "            bike.shape # 중복값 제거 전\n",
    "            bike_dup.shape # 중복값 제거 후 \n",
    "            ```\n",
    "            \n",
    "            전 후의 결과가 동일하므로 중복값이 없다.\n",
    "            \n",
    "            1. 데이터 전처리\n",
    "            \n",
    "            ```python\n",
    "            # 'instant' 열을 제외한 모든 열을 포함하는 새로운 데이터프레임인 'bike_dummy'를 생성\n",
    "            bike_dummy=bike.iloc[:,1:16]\n",
    "            # 'bike_dummy'의 각 열에 대한 고유값의 개수를 내림차순으로 출력하는 반복문 작성\n",
    "            # 코드 실행시 데이터프레임의 각 열에 대한 고유값의 개수 출력\n",
    "            for col in bike_dummy:\n",
    "            \tprint(bike_dummy[col].value_counts(ascending=False),'\\n\\n\\n')\n",
    "            ```\n",
    "            \n",
    "            결과 확인시 junk/unknown이 없다.\n",
    "            \n",
    "            - 코드 분석과 junk/unknown이란 ?\n",
    "            \n",
    "        1. **중복 및 원하지 않는 열 제거**\n",
    "        - contents\n",
    "            \n",
    "            instant, dteday, casual, registered 열 삭제\n",
    "            \n",
    "            ```python\n",
    "            bike.columns # 열확인\n",
    "            # 필요 없는 열 지운 후 bike_new 데이터 프레임 생성 : 원본 데이터 유지를 위해\n",
    "            bike_new=bike[['season', 'yr', 'mnth', 'holiday', 'weekday',\n",
    "                   'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',\n",
    "                   'cnt']]\n",
    "            bike_new.info()\n",
    "            ```\n",
    "            \n",
    "        1. **새로운 더미 변수 만들기**\n",
    "        - contents\n",
    "            \n",
    "            Q. 왜 더미변수를 만드는 걸까 ? (나중에 시간 될 때 찾아봐야…)\n",
    "            \n",
    "            ‘mnth’,’weekday’,’season’,weathersit’ 4가지 범주형 변수를 갖고 dummy variables를 만든다. 위 4개의 칼럼들은 숫자로 이뤄지긴 했지만 대소관계를 나타내는 수치형 데이터는 아니다. 더미 변수를 만들기 전에 우리는 이것들을 범주형 데이터 타입으로 만들어야한다(왜일까… 이유는 나중에 알아보자.)\n",
    "            \n",
    "            ```python\n",
    "            bike_new.info()\n",
    "            \n",
    "            # Convert to 'category' data type\n",
    "            bike_new['season']=bike_new['season'].astype('category')\n",
    "            bike_new['weathersit']=bike_new['weathersit'].astype('category')\n",
    "            bike_new['mnth']=bike_new['mnth'].astype('category')\n",
    "            bike_new['weekday']=bike_new['weekday'].astype('category')\n",
    "            \n",
    "            # category로 잘 바뀌었는지 확인\n",
    "            bike_new.info()\n",
    "            \n",
    "            #pd.get_dummies : 더미 변수 생성\n",
    "            # 더미가 생성된 원래의 변수들을 drop\n",
    "            # drop_first=True : 생성된 각 더미 세트의 첫 번째 더미 변수를 drop\n",
    "            # 다중공선성을 피하기 위해\n",
    "            bike_new = pd.get_dummies(bike_new, drop_first = True)\n",
    "            bike_new.info()\n",
    "            ```\n",
    "            \n",
    "            1. 데이터 분할 \n",
    "            \n",
    "            ```python\n",
    "            # split 전에 확인 \n",
    "            bike_new.shape\n",
    "            bike_new.info()\n",
    "            \n",
    "            from sklearn.model_selection import train_test_split\n",
    "            \n",
    "            # train data set과 test data set가 항상 같은 행을 갖도록 'random_state'를 지정\n",
    "            np.random.seed(0)\n",
    "            # random_state를 특정 숫자로 설정하면 해당 숫자를 시도로 사용하여 난수를 생성\n",
    "            df_train, df_test = train_test_split(bike_new, train_size = 0.70, test_size = 0.30, random_state = 333)\n",
    "            \n",
    "            # df_train의 information 확인\n",
    "            df_train.info()\n",
    "            df_train.shape\n",
    "            df_test.info()\n",
    "            df_test.shape\n",
    "            ```\n",
    "            \n",
    "        1. **EDA를 통한 시각화**\n",
    "        - contents\n",
    "            1. 숫자 변수 시각화\n",
    "            \n",
    "            ```python\n",
    "            df_train.info()\n",
    "            df_train.columns\n",
    "            \n",
    "            # 숫자형 변수만을 사용하여 새로운 데이터프레임을 제작한다. \n",
    "            bike_num = df_train[['temp','atemp','hum','windspeed','cnt']]\n",
    "            sns.pairplot(bike_num, diag_kind = 'kde')\n",
    "            plt.show()\n",
    "            # plt.show()를 통해 나타난 그림을 확인한 결과\n",
    "            ```\n",
    "            \n",
    "            코드 확인 결과 :\n",
    "            \n",
    "            - ‘temp’, ‘atemp’, ‘cnt’ 사이에 선형 상관 관계가 있다.\n",
    "            - ‘temp’는 섭씨 온도, ‘atemp’는 체감 온도, cnt : 총 라이더 수\n",
    "            1. 범주형 변수 시각화 : boxplot으로 6개의 범주형 변수가 종속 변수에 미치는 영향을 확인\n",
    "            \n",
    "            ```python\n",
    "            # 훈련 데이터를 다시 확인\n",
    "            df_train.info()\n",
    "            \n",
    "            # target 변수 'cnt'에 대한 모든 범주형 변수(더미변수 만들기 전)의 시각화\n",
    "            # 각 예측 변수가 target 변수에 어떻게 누적되는지를 확인한다. \n",
    "            plt.figure(figsize=(25,10))\n",
    "            plt.subplot(2,3,1)\n",
    "            sns.boxplot(x='season', y='cnt', data=bike)\n",
    "            plt.subplot(2,3,2)\n",
    "            sns.boxplot(x='mnth', y = 'cnt', data=bike)\n",
    "            plt.subplot(2,3,3)\n",
    "            sns.boxplot(x = 'weathersit', y = 'cnt', data = bike)\n",
    "            plt.subplot(2,3,4)\n",
    "            sns.boxplot(x = 'holiday', y = 'cnt', data = bike)\n",
    "            plt.subplot(2,3,5)\n",
    "            sns.boxplot(x = 'weekday', y = 'cnt', data = bike)\n",
    "            plt.subplot(2,3,6)\n",
    "            sns.boxplot(x = 'workingday', y = 'cnt', data = bike)\n",
    "            plt.show()\n",
    "            ```\n",
    "            \n",
    "            boxplot을 시각화한 자료를 분석하는 법을 더 보완해야…\n",
    "            \n",
    "            1. Correlation Matrix\n",
    "            \n",
    "            ```python\n",
    "            # Correlation Matrix를 활용하여 변수간 상관계수를 확인하자. \n",
    "            plt.figure(figsize=(25,20))\n",
    "            sns.heatmap(bike_new.corr(), annot = True, cmap = \"RdBu\")\n",
    "            plt.show()\n",
    "            ```\n",
    "            \n",
    "            heatmap 특성상 상관관계가 높은 변수들을 보여주는 것 뿐만 아니라 다중공선성이 높은 변수들까지 보여준다. \n",
    "            \n",
    "            선형 모델을 구축하는 동안 Correlation Matrix를 참조하여 VIF 및 p-value와 함께 서로 다른 상관 값을 검증하여 모델에서 선택/제거할 올바른 변수를 식별한다.\n",
    "            \n",
    "        1. **Rescaling the features**\n",
    "        - contents\n",
    "            \n",
    "            Q. MinMaxScaler를 사용하여 모든 feature들을 동일한 스케일로 조정해나가는 이유\n",
    "            \n",
    "            ```python\n",
    "            # MinMaxScaler를 사용하여 모든 feature 들을 동일한 스케일로 조정.\n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "            # Checking the values before scaling\n",
    "            df_train.head()\n",
    "            # 칼럼 확인\n",
    "            df_train.columns\n",
    "            \n",
    "            # 수치형 변수들은 MinMaxScaler를 사용하여 스케일링하는 작업 수행\n",
    "            num_vars = ['temp','atemp','hum','windspeed','cnt']\n",
    "            df_train[num_vars] = scaler.fit_transform(df_train[num_vars])\n",
    "            \n",
    "            # check values after scaling\n",
    "            df_train.head()\n",
    "            \n",
    "            # checking values after scaling\n",
    "            df_train.head()\n",
    "            df_train.describe()\n",
    "            ```\n",
    "            \n",
    "        1. **선형 모델 구축**\n",
    "        - contents\n",
    "            1. Dividing into X and Y sets for the model building\n",
    "            \n",
    "            ```python\n",
    "            # cnt 열을 데이터프레임에서 분리하고 해당 열의 값을 반환. cnt값은 종속 변수로 사용\n",
    "            y_train = df_train.pop('cnt')\n",
    "            X_train = df_train  # X_train은 cnt열을 제외한 모든 열을 포함\n",
    "            ```\n",
    "            \n",
    "            - RFE\n",
    "            \n",
    "            Q. RFE를 왜 사용할까 ?\n",
    "            \n",
    "            재귀적 기능 제거 : RFE(sklearn의 유틸리티인)와의 호환성을 위해 SciKit Learn의 선형회귀함수를 사용할 예정이다. \n",
    "            \n",
    "            ```python\n",
    "            # Importing RFE and LinearRegression\n",
    "            from sklearn.feature_selection import RFE\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            \n",
    "            # 선형회귀 모델 학습하고 RFE 사용하여 가장 중요한 변수들을 선택하는 코드\n",
    "            lm = LinearRegression() # 선형 회귀 모델 생성, 'lm'은 선형회귀 모델임을 나타냄\n",
    "            lm.fit(X_train,y_train)\n",
    "            \n",
    "            rfe = RFE(estimator = lm, n_features_to_select=15)\n",
    "            rfe.fit(X_train,y_train)\n",
    "            \n",
    "            # RFE를 통해 선택된 특성들과 그 선택여부, 그리고 랭킹 정보를 리스트로 출력\n",
    "            list(zip(X_train.columns,rfe.support_,refe.ranking_))\n",
    "            # rfe.support_ : 선택된 특성들을 나타내는 불리언(True/False) 배열이다. \n",
    "            # 선택되면 true, 선택되지 않으면 false\n",
    "            # rfe.ranking_ : 각 특성의 랭킹을 나타내는 배열이다. 숫자가 낮을수록 더 중요한\n",
    "            # 특성으로 간주된다.\n",
    "            \n",
    "            # 선택된 피쳐들의 열 이름 추출\n",
    "            col = X_train.columns[rfe.support_]\n",
    "            col\n",
    "            # 선택되지 못한 피쳐들의 열 이름 추출\n",
    "            X_train.columns[~rfe.support_]\n",
    "            # 선택된 특성들로 이뤄진 col 변수를 사용하여 X_train에서 선정된 칼럼으로만 이루어진\n",
    "            # 데이터프레임을 생성\n",
    "            X_train_rfe = X_train[col]\n",
    "            ```\n",
    "            \n",
    "        1. **‘STATS MODEL’을 사용하여 선형 모델 구축**\n",
    "        - contents\n",
    "            1. Model 1\n",
    "            \n",
    "            ```python\n",
    "            from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "            \n",
    "            # VIF Check\n",
    "            vif = pd.DataFrame()\n",
    "            vif['Features'] = X_train_rfe.columns\n",
    "            vif['VIF'] = [variance_inflation_facotr(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\n",
    "            vif['VIF'] = round(vif['VIF'],2)\n",
    "            vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "            vif\n",
    "            # 위 코드를 실행시키면 features 별로 VIF 수치가 나온다. \n",
    "            \n",
    "            import statsmodels.api as sm\n",
    "            # 상수항 추가\n",
    "            X_train_lm1 = sm.add_constant(X_train_rfe)\n",
    "            # Create a first fitted model\n",
    "            lr1 = sm.OLS(y_train,X_train_lm1).fit()\n",
    "            \n",
    "            # Check the parameters obtained\n",
    "            # lr1의 회귀 계수를 나타낸다. \n",
    "            lr1.params\n",
    "            \n",
    "            # 통계장표 출력\n",
    "            print(lr1.summary())\n",
    "            ```\n",
    "            \n",
    "            1. Model 2\n",
    "            - high p-value 및 high VIF를 기반으로 ‘atemp’ 변수를 제거\n",
    "            \n",
    "            ```python\n",
    "            X_train_new = X_train_rfe.drop([\"atemp\"],axis = 1)\n",
    "            \n",
    "            # Check for the VIF values of the feature variables.\n",
    "            from statsmodels.stats.outliers_influence import varaiance_inflation_factor\n",
    "            \n",
    "            # VIF 동일\n",
    "            # ...\n",
    "            \n",
    "            # Add a constant \n",
    "            X_train_lm2 = sm.add_constant(X_train_new)\n",
    "            \n",
    "            # Create a first fitted model\n",
    "            lr2 = sm.OLS(y_train, X_train_lm2).fit()\n",
    "            # lr2의 회귀계수 나타내기\n",
    "            lr2.params\n",
    "            \n",
    "            # 통계장표 출력\n",
    "            print(lr2.summary())\n",
    "            ```\n",
    "            \n",
    "            1. Model 3\n",
    "            - ‘VIF’ 값을 기준으로 변수 ‘hum’을 제거한다.\n",
    "            - ‘hum’의 VIF가 두 번째로 높긴 하지만, 우리는 온도가 자전거 대여와 같은 사업에 중요한 요소가 될 수 있다는 일반적인 지식을 기반으로 하여 ‘temp’가 아닌 ‘hum’을 떨군다.\n",
    "            \n",
    "            ```python\n",
    "            X_train_new = X_train_new.drop([\"hum\"],axis = 1)\n",
    "            \n",
    "            # VIF Check\n",
    "            # Add constant \n",
    "            # create a first fitted model\n",
    "            \n",
    "            lr3.params\n",
    "            print(lr3.summary())\n",
    "            ```\n",
    "            \n",
    "            1. Model 4\n",
    "            - Model 3와 마찬가지로 온도는 자전거 대여와 같은 사업에 중요하기 때문에 ‘temp’가 아닌 ‘season3’ 변수를 제거한다. (코드 생략)\n",
    "            1. Model 5\n",
    "            - 매우 높은 p- 값을 제거한다. mnth_10\n",
    "            \n",
    "            ```python\n",
    "            X_train_new = X_train_new.drop([\"mnth_10\"], axis = 1)\n",
    "            \n",
    "            # VIF check\n",
    "            # add a constant \n",
    "            \n",
    "            # code 생략...\n",
    "            \n",
    "            lr3.parmas\n",
    "            ```\n",
    "            \n",
    "            1. Model 6\n",
    "            - 높은 p- 값을 기준으로 ‘mnth_3’ 변수를 제거\n",
    "            \n",
    "            최종 Model 6 : 예측 변수와 모든 예측 변수의 p- 값 사이에 매우 낮은 다중 공선성이 유의한 것으로 보이기 때문에 이 모형은 양호해 보인다.\n",
    "            \n",
    "        1. **최종 모델 해석**\n",
    "        - contents\n",
    "            1. 가설 검정\n",
    "                1. lr6 회귀 모델의 각 독립 변수에 대한 회귀 계수 값\n",
    "                    \n",
    "                    해당 독립 변수가 종속 변수에 얼마나 큰 방향을 미치는지와 그 방향\n",
    "                    \n",
    "                2. F 통계\n",
    "                    \n",
    "                    모형의 전반적인 유의성을 검정하는데 사용된다\n",
    "                    \n",
    "            2. lr6 모델 기반의 최적의 회귀 방정식\n",
    "            3. 상관계수 해석\n",
    "        1. **가정**\n",
    "        - contents\n",
    "            1. 오차항은 일반적으로 평균 0(X,Y가 아님)으로 분포된다.\n",
    "            - train 데이터 잔차 분석 : 예측 오차 계산 후 이를 히스토그램으로 시각화\n",
    "            \n",
    "            ```python\n",
    "            # 선형 회귀 모델인 lr6를 사용하여 훈련 데이터 셋인 X_train_lm6에서 예측을 수행하고\n",
    "            # 예측갑을 y_train_pred 변수에 저장한다. \n",
    "            y_train_pred = lr6.predict(X_train_lm6)\n",
    "            \n",
    "            # 선형회귀모델의 예측 오차를 계산하고 이를 히스토그램으로 시각화한다. \n",
    "            res = y_train - y_train_pred  # 예측 오차 계산\n",
    "            # plot the histogram of the error terms\n",
    "            fig = plt.figuree()\n",
    "            sns.distplot((res), bins = 20)\n",
    "            fig.suptitle('Error Terms', fontsize = 20)\n",
    "            plt.xlabel('Errors', fontsize = 18)\n",
    "            ```\n",
    "            \n",
    "            1. X와 Y 사이에 선형 관계가 있다. \n",
    "            \n",
    "            ```python\n",
    "            bike_new = bike_new[['temp','atemp','hum','windspeed','cnt']]\n",
    "            \n",
    "            sns.pairplot(bike_num, diag_kind = 'kde')\n",
    "            plt.show()\n",
    "            ```\n",
    "            \n",
    "            ‘temp’와 ‘atemp’가 종속 변수 ‘cnt’와 선형관계가 있음을 알 수 있다. \n",
    "            \n",
    "            1. 에측 변수 사이의 다중 공선성 유무 파악\n",
    "            \n",
    "            ```python\n",
    "            from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "            \n",
    "            # create a dataframe that will contain the names of all the feature var and VIF\n",
    "            vif = pd.DataFrame()\n",
    "            vif['Featrues'] = X_train_new.columns\n",
    "            vif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\n",
    "            vif['VIF'] = round(vif['VIF'],2)\n",
    "            vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "            vif\n",
    "            ```\n",
    "            \n",
    "        1. **최종 모델을 사용한 예측**\n",
    "        - contents\n",
    "            \n",
    "            모델을 적합시키고 가정을 확인했으므로, 최종 모델(lr6)을 사용하여 예측할 시간이다. \n",
    "            \n",
    "            1. test sets에 스케일링 적용\n",
    "            \n",
    "            ```python\n",
    "            # 수치형 변수 스케일링\n",
    "            num_vars = ['temp','atemp','hum','windspeed','cnt']\n",
    "            df_test[num_vars] = scaler.transform(df_test[num_vars])\n",
    "            \n",
    "            # 스케일링이 잘 적용됬는지를 확인\n",
    "            df_test.head()\n",
    "            df_test.describe()\n",
    "            ```\n",
    "            \n",
    "            1. X_test와 y_test로 구분\n",
    "            \n",
    "            ```python\n",
    "            y_test = df_test.pop('cnt')\n",
    "            X_test = df_test\n",
    "            X_test.info()\n",
    "            \n",
    "            # X_test에 대한 일련의 데이터 전처리 작업을 수행\n",
    "            # 테스트 데이터를 모델에 입력하기 전에 필요한 변환을 적용\n",
    "            # Selecting the variables that were part of final model\n",
    "            col1 = X_train_new.columns\n",
    "            X_test = X_test[col1]\n",
    "            # adding constant variable to test dataframe\n",
    "            X_test_lm6 = sm.add_constant(X_test)\n",
    "            X_test_lm6.info()\n",
    "            \n",
    "            # 선형 회귀 모델인 lr6를 사용하여 X_test_lm6에 대한 예측을 수행하고, 예측값을\n",
    "            # y_pred 변수에 저장한다. \n",
    "            y_pred = lr6.predict(X_test_lm6)\n",
    "            ```\n",
    "            \n",
    "        1. **모델 평가**\n",
    "        - contents\n",
    "            \n",
    "            ```python\n",
    "            # Plotting y_test and y_pred to understand the spread\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            plt.scatter(y_test, y_pred, alpha = .5)\n",
    "            fig.suptitle('y_test vs y_pred', fontsize = 20) # plot 제목\n",
    "            plt.xlabel('y_test', fontsize = 18) # X-label\n",
    "            plt.ylabel('y_pred', fontsize = 16) \n",
    "            plt.show()\n",
    "            ```\n",
    "            \n",
    "            - test에 대한 R^2 값과 adjusted R^2 값\n",
    "            \n",
    "            ```python\n",
    "            from sklearn.metrics import r2_score\n",
    "            r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Test에 대한 조정된 R^2 값\n",
    "            X_test.shape\n",
    "            \n",
    "            # n = 행의 수\n",
    "            n = X_test.shape\n",
    "            # p is the shape along axis 1\n",
    "            p = X_test.shape[1]\n",
    "            \n",
    "            adjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "            adjusted_r2\n",
    "            ```\n",
    "            \n",
    "        1. **최종 결과 비교**\n",
    "        - contents\n",
    "            \n",
    "            Q . 왜 다음과 같은 결과가 세트를 일반화할 수 있는 좋은 모델일까 ?\n",
    "            \n",
    "            - Train R^2 :0.824\n",
    "            - Train Adjusted R^2 :0.821\n",
    "            - Test R^2 :0.820\n",
    "            - Test Adjusted R^2 :0.812\n",
    "            - 이것은 다양한 데이터 세트를 매우 잘 '일반화'할 수 있는 정말 좋은 모델인 것 같습니다.\n",
    "        1. **Fin Report**\n",
    "        - contents\n",
    "            \n",
    "            최종 모델에 따르면, 자전거 예약에 영향을 미치는 상위 3개의 예측 변수는 다음과 같다.\n",
    "            \n",
    "            1. temp   2. weathersit_3   3. yr\n",
    "            \n",
    "            따라서 최대 예약을 달성하기 위해 계획을 수립하는 동안 이러한 변수를 가장 중요하게 고려하는 것이 좋다. \n",
    "            \n",
    "            1. season_4   2. 풍속"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

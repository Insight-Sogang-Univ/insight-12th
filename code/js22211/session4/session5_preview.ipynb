{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da96d6d0-123f-4416-be10-d98cac9f4486",
   "metadata": {},
   "source": [
    "### Session 5 : Theory\n",
    "\n",
    "- contents\n",
    "    \n",
    "    **[ Review : Regression ]** \n",
    "    \n",
    "    - contents\n",
    "        1. 회귀란 ?\n",
    "        \n",
    "        정의 : 지도학습의 한 종류로 연속적인 데이터에서 패턴을 찾아내는 통계적인 방법\n",
    "        \n",
    "        종류 : 선형회귀, 비선형회귀, 로지스틱 회귀, 릿지회귀, 라쏘회귀, 다항회귀\n",
    "        \n",
    "        활용 : 데이터 요약, 예측, 시계열 모델링\n",
    "        \n",
    "        평가지표 : MSE, MAE, R-square, Adjusted R-square, AIC, BC\n",
    "        \n",
    "        회귀는 데이터분석의 유용한 통계적 도구이자, ML/DL 이해의 밑바탕\n",
    "        \n",
    "        1. 단순/다중선형회귀분석\n",
    "            1. 단순선형회귀분석\n",
    "            2. 다중선형회귀분석\n",
    "        \n",
    "        두 종류는 설명변수의 개수에 차이가 있을 뿐, 두 회귀분석은 본질적으로 같다. \n",
    "        \n",
    "        → 그렇다면 위와 같은 최적의 회귀선을 찾는 법은 ? \n",
    "        \n",
    "        1. 최소 제곱법, 최소 자승법 (least squares method, ordinary least square) \n",
    "        \n",
    "        보통 OLS라고 한다. 잔차의 제곱을 최소화하는 회귀선을 찾는 방법이다. \n",
    "        \n",
    "    \n",
    "    **[ 선형 회귀 ]**\n",
    "    \n",
    "    - contents\n",
    "        1. 다중선형회귀의 수식적인 이해.\n",
    "        \n",
    "        종속 변수 y에 대해, 영향을 줄 것 같은 여러 설명변수를 채택하여 선형적으로 식을 작성한 모습이다 . \n",
    "        \n",
    "        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee620b36-4663-419f-bd1c-683a8774e63e/c0c7fd0b-c7db-4cc9-84e4-72c6079f557d/image.png)\n",
    "        \n",
    "        상수항과 오차를 제외하면, 밑의 그림처럼 작성할 수 있다. \n",
    "        \n",
    "        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee620b36-4663-419f-bd1c-683a8774e63e/b6e7cba0-7eb4-4129-a2cc-38e33e76f340/image.png)\n",
    "        \n",
    "        - 선형회귀식은 결국 종속변수 y에 대한 설명변수의 가중평균이라고 할 수 있다.\n",
    "        - 설명변수에 가중치를 곱해 더함으로써 y를 얼마나 잘 설명하는 지를 나타낸다.\n",
    "        \n",
    "        그러나 선형회귀를 모든 상황에 쓸 수 있는 것은 아니다. \n",
    "        \n",
    "        1. 다중선형회귀의 기본가정\n",
    "        \n",
    "        선형 회귀를 사용하기 위해선 다음의 가정이 필요하다. 즉, 다음의 가정을 만족하지 않으면 분석 결과의 신뢰성이 떨어질 수 밖에 없고, 심각한 경우는 분석이 아예 불가능하다 .\n",
    "        \n",
    "        - 선형성 : 종속변수와 설명변수 간 관계가 선형적이여야 한다.\n",
    "        - 오차항의 평균은 0이다.\n",
    "        - 독립성 : 각각의 설명변수가 서로 선형독립적이여야 한다. 선형독립적이지 않은 경우 다중공선성이 있다고 한다.\n",
    "        - 등분산성 : 오차항(백색 잡음)의 분산이 일정해야 한다. 오차항의 분산이 일정하지 않은 경우 이분산성이 있다고 한다. 등분산성은 1) 잔차의 도표화와 2) 검정을 통해 알아볼 수 있다.\n",
    "        - 오차항은 자기상관되어 있지 않다 . : 오차의 공분산은 항상 0이어야 한다. 오차항의 공분산이 0이 아닌 경우, 자기상관이 있다고 한다. 특히 시계열 자료에서 많이 나타난다. 자기상관은 1) 더빈-왓슨 검정 2) ACF, PACF 함수를 찍어봄으로써 확인할 수 있다.\n",
    "        - 정규성 : 오차항이 정규분포를 따른다. 정규성을 알아보는 법은 다음과 같다. 1) 샤피로-윌크 검정 2) 자퀴-베라 검정 3) Q-Q plot 찍어보기\n",
    "        1. 회귀분석 평가방법\n",
    "        \n",
    "        회귀분석의 결과가 얼마나 유효한지 평가하기 위해선 회귀선을 데이터와 함께 시각화하거나 통계지표를 이용하면 된다. \n",
    "        \n",
    "        1. 시각화 \n",
    "        2. 통계지표 : R-square과 이를 보완한 몇가지 지표에 대해 설명\n",
    "        \n",
    "        <1> 결정계수 R-squared \n",
    "        \n",
    "        SST ( total sum of squares ) : 총 제곱합으로 종속 변수의 실제값과 평균 간의 차이를 제곱하여 합한 값이다.. \n",
    "        SSR ( Regression Sum of squares ) : 회귀 제곱합으로, 예측 값과 평균 간의 차이를 제곱하여 합한 값이다. 이는 회귀 모델에 의해 설명된 변동량을 나타낸다 . \n",
    "        SSE ( Error sum of squares ) : 잔차 제곱합으로, 실제 값과 예측 값 간의 차이를 제곱하여 합한 값이다. 이는 회귀 모델이 설명하지 못한 변동량, 오차를 나타낸다. \n",
    "        \n",
    "        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee620b36-4663-419f-bd1c-683a8774e63e/44da4f67-9a4b-45ff-b1ca-723eb352e4f9/image.png)\n",
    "        \n",
    "        R^2는 결정 계수의 약어로, 회귀 분석에서 모델이 설명하는 데이터의 총 변동(평균과의 차이)에서 설명된 비율을 나타낸다. \n",
    "        \n",
    "        초록색 선 : 실제값 - 평균 , 보라색 선 : 실측값 - 평균 , 빨간색 선 : 실측값 - 예측값 \n",
    "        \n",
    "        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee620b36-4663-419f-bd1c-683a8774e63e/b6c45971-d0f8-469c-b92e-a3ab74020177/image.png)\n",
    "        \n",
    "        모델이 데이터를 얼마나 잘 설명하는지를 측정하는 지표로, 0과 1 사이의 값으로 나타난다.\n",
    "        \n",
    "        나중에 시간 되면… 수학적인 이해가 좀 필요할 듯… \n",
    "        \n",
    "        <2> 조정된 결정 계수 (adjusted R-squared, Adjusted R^2)\n",
    "        \n",
    "        R^2는 설명변수의 개수가 증가하면 증가할 수록 자연스레 증가한다. → 데이터와 큰 관련성 없는 변수를 추가해도 R-square 값은 높아진다.\n",
    "        \n",
    "        따라서 변수의 개수 증가에 덜 민감하도록 조정한 지표가 adjusted R^2이다. 이는 기존 R^2의 식에 (n-1/n-k)를 곱한 것을 변수의 개수에 따른 penalty를 부여한다 .\n",
    "        \n",
    "        <3> information criteria : 값이 낮을 수록 좋다.\n",
    "        \n",
    "        - AIC : Akaike information criterion\n",
    "            - BIC에 비해 복잡성에 대한 패널티가 비교적 작다.\n",
    "        - BIC : Bayesian Information Criterion\n",
    "            - AIC보다 더 엄격한 기준으로 , 데이터의 양에 따라 더 강한 패널티를 부과한다.\n",
    "    \n",
    "    **[ 비선형 회귀 ]**\n",
    "    \n",
    "    - contents\n",
    "        1. 다항식 회귀 모델\n",
    "        \n",
    "        다항식을 이용한 회귀 모델은 선형 모델의 feature를 다항식으로 만들어 선형 회귀를 사용한다. \n",
    "        \n",
    "        기존 변수들을 다항식으로 만들고 sklearn의 선형회귀 클래스를 똑같이 이용한다. \n",
    "        \n",
    "        ```python\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        \n",
    "        # 다항식 변환기를 초기화, 기본 설정은 2차항을 포함, 상수항을 추가. 이 클래스는 주어진 \n",
    "        # 입력 변수를 다항식 형태로 변환할 준비를 한다 .\n",
    "        poly = PolynomialFeatures() \n",
    "        # fit method는 입력 데이터를 바탕으로 다항식 변환 규칙을 학습한다.\n",
    "        # [2,3]이라는 2차원 벡터에 대해 2차항까지 변환할 수 있는 구조를 학습한다. \n",
    "        poly.fit([[2,3]])\n",
    "        poly = PolynomailFeatures(include_bias = False) # 상수항 제외\n",
    "        # train_input 데이터를 기반으로 다항식 변환 규칙을 학습한다.\n",
    "        # 입력 변수의 다항식 변환을 위한 구조가 결정된다.\n",
    "        poly.fit(train_input)\n",
    "        # transform method에서는 test_input을 다항식 형태로 변환하며, 그 결과를 test_poly에 \n",
    "        # 저장한다. 이 단계에서 실제 예측에 사용할 테스트 데이터가 다항식으로 변환된다. \n",
    "        test_poly = poly.transform(test_input)\n",
    "        \n",
    "        # 선형 회귀 모델을 초기화한다. 다항식으로 변환될 데이터를 이용해 회귀 분석을 수행할 준비\n",
    "        # 를 한다.\n",
    "        lr = LinearRegression()\n",
    "        # 변환된 훈련 데이터 train_poly와 해당 목표ㄱ밧 train_target을 사용해 선형 회귀 모델\n",
    "        # 을 학습시킨다. \n",
    "        lr.fit(train_poly, train_target)\n",
    "        print(lr.score(train_poly, train_target))\n",
    "        ```\n",
    "        \n",
    "        code analysis : PolynomialFeatures를 사용해 기존 데이터를 다항식으로 변환하고, 변환된 데이터를 LinearRegression을 이용해서 회귀 분석한다.\n",
    "        \n",
    "        1. 지수/로그 회귀 모델\n",
    "        \n",
    "        어떤 칼럼의 증감 형태가 지수적일때 or 그 역일때 회귀선을 그리고 싶다면 지수,로그 식을 이용해야 한다 . \n",
    "        \n",
    "        다항식의 경우처럼, 기존식에 지수, 로그를 사용해 변형시킨 후 선형회귀를 이용한다.\n",
    "        \n",
    "        ```python\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.datasets import load_breast_cancer\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        \n",
    "        # 산점도 그리기\n",
    "        plt.scatter(cancer.data[:,col1], cancer.data[:,col2], c= cancer.target, alpha=0.3)\n",
    "        # col1을 입력 변수로 사용하여 회귀분석을 수행할 것\n",
    "        X = cancer.data[:,[col1]]\n",
    "        # col2에 해당하는 데이터를 로그 변환한 결과이다. 로그 변환은 값이 지수적으로 변화할 때 \n",
    "        # 선형 회귀로 모델링할 수 있게 만들어준다. + 0.01은 로그 변환 시 값이 0이 되는 것을 \n",
    "        # 방지하기 위해 추가되는 값이다.  \n",
    "        **y = np.log(cancer.data[:,col2]+0.01)**\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        # fit method를 사용해 회귀 모델을 X와 y에 맞춰 학습시킨다. 여기서 지수적 관계를 선형\n",
    "        # 회귀로 표현하기 위해 종속 변수를 로그로 변환하여 학습한다. \n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # 0에서 30까지 0.1 간격으로 값을 생성하여 xs에 저장한다. 이 값들은 회귀선을 그릴 때\n",
    "        # 사용할 입력값들이다.\n",
    "        xs = np.arange(0,30,0.1)\n",
    "        # model.coef_는 학습된 선형 회귀 모델의 기울기를 나타내고, model.intercept_는 절편이다.\n",
    "        # 이를 사용해 지수 함수를 적용하여 예측 값을 만든다. np.exp()는 로그 변환된 데이터를\n",
    "        # 다시 지수적 형태로 되돌린다. \n",
    "        ys = np.exp(xs*model.coef_[0] + model.intercept_) - 0.01\n",
    "        \n",
    "        plt.scatter(cancer.data[:,col1], cancer.data[:,col2], c= cancer.target, alpha=0.3)\n",
    "        plt.plot(xs,ys,'r-',lw=3)\n",
    "        ```\n",
    "        \n",
    "        code analysis : 특정 칼럼의 값이 지수적인 변동을 보일 때, 로그 변환을 사용하여 선형 회귀 모델을 적용하는 방식이다. 로그 변환을 통해 종속 변수를 선형적으로 만들고, 학습된 모델을 다시 지수 형태로 변환하여 예측 결과를 도출한다.\n",
    "\n",
    "       참고 ) 이미지는 강의자료의 이미지입니다. : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0f757-9337-4933-88a4-2c3a3bb445b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
